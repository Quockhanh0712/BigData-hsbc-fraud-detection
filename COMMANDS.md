# üìã HSBC Fraud Detection - Command Cheat Sheet

Quick reference cho c√°c l·ªánh th∆∞·ªùng d√πng.

---

## üöÄ KH·ªûI ƒê·ªòNG H·ªÜ TH·ªêNG

### C√°ch 1: S·ª≠ d·ª•ng Makefile (ƒê∆°n gi·∫£n nh·∫•t)
```powershell
make start      # Kh·ªüi ƒë·ªông t·∫•t c·∫£ services
make setup      # Setup database
make train      # Train model
make stream     # Start streaming
```

### C√°ch 2: S·ª≠ d·ª•ng Automation Script
```powershell
.\scripts\automation.ps1 start
.\scripts\automation.ps1 setup
.\scripts\automation.ps1 train
.\scripts\automation.ps1 stream
```

### C√°ch 3: Docker Compose Manual
```powershell
docker compose up -d
docker exec -it cassandra cqlsh -e "CREATE KEYSPACE..."
docker exec spark-master /opt/spark/bin/spark-submit...
```

---

## üîç KI·ªÇM TRA TR·∫†NG TH√ÅI

### Xem t·∫•t c·∫£ containers
```powershell
docker compose ps
# ho·∫∑c
make status
# ho·∫∑c
.\scripts\automation.ps1 status
```

### Health check to√†n h·ªá th·ªëng
```powershell
make health
# ho·∫∑c
.\scripts\automation.ps1 health
```

### Ki·ªÉm tra resource usage
```powershell
docker stats
```

---

## üìä XEM LOGS

### Logs c·ªßa t·∫•t c·∫£ services
```powershell
docker compose logs -f
# ho·∫∑c
make logs
```

### Logs c·ªßa service c·ª• th·ªÉ
```powershell
# Producer (xem transactions ƒëang g·ª≠i)
docker logs -f producer --tail 100

# Spark Master (xem batch processing)
docker logs -f spark-master --tail 200

# API (xem requests)
docker logs -f api --tail 50

# Dashboard
docker logs -f dashboard --tail 30

# Ho·∫∑c d√πng Makefile
make logs-service SERVICE=producer
```

### Search trong logs
```powershell
# T√¨m fraud detections
docker logs spark-master 2>&1 | Select-String "fraud"

# T√¨m errors
docker logs api 2>&1 | Select-String "ERROR"

# T√¨m batch processing
docker logs spark-master 2>&1 | Select-String "Batch"
```

---

## üíæ CASSANDRA COMMANDS

### K·∫øt n·ªëi CQL Shell
```powershell
docker exec -it cassandra cqlsh
```

### Queries th∆∞·ªùng d√πng
```powershell
# ƒê·∫øm t·ªïng fraud alerts
docker exec -it cassandra cqlsh -e "SELECT COUNT(*) FROM hsbc.fraud_alerts;"

# Xem 10 alerts g·∫ßn nh·∫•t
docker exec -it cassandra cqlsh -e "SELECT transaction_id, amount, category, merchant, detected_at FROM hsbc.fraud_alerts LIMIT 10;"

# Query theo category
docker exec -it cassandra cqlsh -e "SELECT * FROM hsbc.fraud_alerts WHERE category='grocery_pos' LIMIT 5 ALLOW FILTERING;"

# Ho·∫∑c d√πng Makefile
make cassandra-query
```

### X√≥a d·ªØ li·ªáu
```powershell
# Truncate table
docker exec -it cassandra cqlsh -e "TRUNCATE hsbc.fraud_alerts;"

# Drop v√† recreate table
docker exec -it cassandra cqlsh -e "DROP TABLE IF EXISTS hsbc.fraud_alerts;"
make setup  # T·∫°o l·∫°i table
```

---

## üì® KAFKA COMMANDS

### List topics
```powershell
docker exec kafka kafka-topics --list --bootstrap-server localhost:9092
```

### Describe topic
```powershell
docker exec kafka kafka-topics --describe --topic transactions_hsbc --bootstrap-server localhost:9092
```

### Consume messages (xem transactions)
```powershell
# Xem 10 messages ƒë·∫ßu ti√™n
docker exec kafka kafka-console-consumer \
  --topic transactions_hsbc \
  --from-beginning \
  --bootstrap-server localhost:9092 \
  --max-messages 10

# Consume real-time
docker exec kafka kafka-console-consumer \
  --topic transactions_hsbc \
  --bootstrap-server localhost:9092
```

### Consumer groups
```powershell
# List groups
docker exec kafka kafka-consumer-groups --list --bootstrap-server localhost:9092

# Describe group
docker exec kafka kafka-consumer-groups --describe \
  --group spark-kafka-streaming \
  --bootstrap-server localhost:9092
```

---

## ‚ö° SPARK COMMANDS

### Spark UI URLs
```powershell
# Spark Master UI
start http://localhost:8080
# ho·∫∑c
make spark-ui

# Spark Application UI (khi job ƒëang ch·∫°y)
start http://localhost:4040
```

### Submit Streaming Job
```powershell
# C√°ch 1: Makefile
make stream

# C√°ch 2: Automation script
.\scripts\automation.ps1 stream

# C√°ch 3: Manual
docker cp streaming-pipeline/unified_streaming.py spark-master:/opt/spark-apps/
docker exec spark-master /opt/spark/bin/spark-submit \
  --master spark://spark-master:7077 \
  --deploy-mode client \
  --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,com.datastax.spark:spark-cassandra-connector_2.12:3.4.0,org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262 \
  /opt/spark-apps/unified_streaming.py
```

### Stop Streaming Job
```powershell
docker exec spark-master pkill -f unified_streaming.py
```

### Restart Spark Cluster
```powershell
docker compose restart spark-master spark-worker
Start-Sleep -Seconds 30
```

---

## ü§ñ MODEL TRAINING

### Train XGBoost Model
```powershell
# C√°ch 1: Makefile
make train

# C√°ch 2: Automation script
.\scripts\automation.ps1 train

# C√°ch 3: Manual
# Install XGBoost first
docker exec spark-master bash -c "pip3 install xgboost scikit-learn pyarrow"

# Copy files
docker cp streaming-pipeline/model_retraining_xgb.py spark-master:/opt/spark-apps/
docker cp streaming-pipeline/feature_engineering.py spark-master:/opt/spark-apps/

# Train (local mode recommended for training)
docker exec spark-master bash -c "cd /opt/spark-apps && export PYSPARK_PYTHON=/usr/bin/python3 && /opt/spark/bin/spark-submit --master local[4] --driver-memory 4g --conf spark.sql.shuffle.partitions=20 /opt/spark-apps/model_retraining_xgb.py"
```

### Verify Model
```powershell
# Check XGBoost model exists
docker exec spark-master ls -lh /opt/data/models/fraud_xgb_21features

# View metadata
docker exec spark-master cat /opt/data/models/fraud_xgb_21features/metadata/part-00000

# Check XGBoost version
docker exec spark-master python3 -c "import xgboost; print('XGBoost version:', xgboost.__version__)"
```

---

## üåê API COMMANDS

### Health Check
```powershell
curl http://localhost:8000/
```

### Get Fraud Alerts
```powershell
# Get 10 latest alerts
curl http://localhost:8000/fraud/alerts?limit=10

# Filter by category
curl "http://localhost:8000/fraud/alerts?category=grocery_pos&limit=20"

# Filter by state
curl "http://localhost:8000/fraud/alerts?state=CA&limit=15"

# PowerShell formatted output
(curl http://localhost:8000/fraud/alerts?limit=5).Content | ConvertFrom-Json | ConvertTo-Json -Depth 3
```

### Get Statistics
```powershell
curl http://localhost:8000/fraud/stats

# PowerShell formatted
(curl http://localhost:8000/fraud/stats).Content | ConvertFrom-Json | ConvertTo-Json -Depth 3
```

### Get Total Count
```powershell
curl http://localhost:8000/fraud/count
```

### API Docs
```powershell
start http://localhost:8000/docs
# ho·∫∑c
make api-docs
```

### Test All Endpoints
```powershell
make api-test
```

---

## üìä DASHBOARD

### M·ªü Dashboard
```powershell
start http://localhost:8501
# ho·∫∑c
make dashboard
```

### Restart Dashboard
```powershell
docker compose restart dashboard
```

---

## üíæ MINIO COMMANDS

### MinIO Console
```powershell
start http://localhost:9001
# Login: admin / password123
```

### List Buckets
```powershell
docker exec minio mc ls myminio/
```

### List Files
```powershell
# List all files in bucket
docker exec minio mc ls -r myminio/hsbc-data/

# List archived streams
docker exec minio mc ls -r myminio/hsbc-data/stream-archive/

# List models
docker exec minio mc ls myminio/hsbc-data/models/
```

### Disk Usage
```powershell
docker exec minio mc du myminio/hsbc-data/
```

---

## üîÑ UPDATE CODE

### Update Streaming Pipeline
```powershell
# 1. Edit file locally
# 2. Stop streaming job
docker exec spark-master pkill -f unified_streaming.py

# 3. Copy updated file
docker cp streaming-pipeline/unified_streaming.py spark-master:/opt/spark-apps/
docker cp streaming-pipeline/feature_engineering.py spark-master:/opt/spark-apps/

# 4. Restart streaming
make stream
```

### Update API
```powershell
# 1. Edit api/main.py or api/database.py locally
# 2. Rebuild and restart
docker compose build api
docker compose restart api

# 3. Test
curl http://localhost:8000/
```

### Update Dashboard
```powershell
# 1. Edit dashboard/app.py locally
# 2. Rebuild and restart
docker compose build dashboard
docker compose restart dashboard

# 3. Refresh browser
start http://localhost:8501
```

---

## üîß PRODUCER CONTROL

### Start/Stop Producer
```powershell
# Stop (ng·ª´ng g·ª≠i transactions)
docker compose stop producer

# Start (ti·∫øp t·ª•c g·ª≠i)
docker compose start producer

# Restart
docker compose restart producer
```

### Change Transaction Rate
```powershell
# Edit docker-compose.yml:
# producer:
#   environment:
#     TRANSACTION_RATE: 5  # Thay ƒë·ªïi t·ª´ 2 ‚Üí 5

# Restart producer
docker compose restart producer
```

### View Producer Logs
```powershell
docker logs -f producer --tail 100
```

---

## üõë STOP/RESTART SYSTEM

### Stop All (gi·ªØ data)
```powershell
docker compose stop
# ho·∫∑c
make stop
# ho·∫∑c
.\scripts\automation.ps1 stop
```

### Restart All
```powershell
docker compose restart
# ho·∫∑c
make restart
# ho·∫∑c
.\scripts\automation.ps1 restart
```

### Down (x√≥a containers, gi·ªØ volumes)
```powershell
docker compose down
# ho·∫∑c
make down
```

### Complete Cleanup (X√ìA T·∫§T C·∫¢)
```powershell
docker compose down -v
# ho·∫∑c
make clean
# ho·∫∑c
.\scripts\automation.ps1 clean
```

---

## üîç TROUBLESHOOTING QUICK FIXES

### Container kh√¥ng start
```powershell
docker logs <container_name>
docker compose restart <service_name>
```

### Spark job kh√¥ng ch·∫°y
```powershell
docker compose restart spark-master spark-worker
Start-Sleep -Seconds 30
# Submit l·∫°i job
```

### Kafka connection failed
```powershell
docker compose restart zookeeper kafka
Start-Sleep -Seconds 30
```

### Cassandra not ready
```powershell
docker compose restart cassandra
Start-Sleep -Seconds 60
docker exec cassandra cqlsh -e "SELECT now() FROM system.local;"
```

### API/Dashboard kh√¥ng c√≥ data
```powershell
# Check streaming job
docker logs spark-master --tail 50

# Check Cassandra data
docker exec cassandra cqlsh -e "SELECT COUNT(*) FROM hsbc.fraud_alerts;"

# Restart API & Dashboard
docker compose restart api dashboard
```

### Port conflicts
```powershell
# T√¨m process ƒëang d√πng port
netstat -ano | Select-String "8000"

# Kill process ho·∫∑c change port trong docker-compose.yml
```

### Complete Reset
```powershell
# 1. Clean everything
docker compose down -v
docker system prune -a --volumes -f

# 2. Rebuild
docker compose build --no-cache

# 3. Start
make start
make setup
make train
make stream
```

---

## üìñ HELPFUL ALIASES (Add to PowerShell Profile)

Th√™m v√†o `$PROFILE`:

```powershell
# HSBC Fraud Detection Aliases
function hsbc-start { Set-Location A:\hsbc-fraud-detection-new; make start }
function hsbc-stop { Set-Location A:\hsbc-fraud-detection-new; make stop }
function hsbc-status { Set-Location A:\hsbc-fraud-detection-new; make status }
function hsbc-logs { Set-Location A:\hsbc-fraud-detection-new; make logs }
function hsbc-health { Set-Location A:\hsbc-fraud-detection-new; make health }
function hsbc-dashboard { start http://localhost:8501 }
function hsbc-api { start http://localhost:8000/docs }
function hsbc-spark { start http://localhost:8080 }
```

Reload profile:
```powershell
. $PROFILE
```

S·ª≠ d·ª•ng:
```powershell
hsbc-start
hsbc-status
hsbc-dashboard
```

---

## üéØ COMMON WORKFLOWS

### Daily Startup
```powershell
make start
Start-Sleep -Seconds 60
make stream
make dashboard
```

### Check System Health
```powershell
make health
make api-test
make cassandra-query
```

### Monitor Real-time
```powershell
# Terminal 1: Streaming logs
docker logs -f spark-master | Select-String "Batch|fraud"

# Terminal 2: API requests
docker logs -f api

# Browser: Dashboard
start http://localhost:8501
```

### Update & Redeploy
```powershell
# 1. Update code
# 2. Stop streaming
docker exec spark-master pkill -f unified_streaming.py

# 3. Copy files
docker cp streaming-pipeline/unified_streaming.py spark-master:/opt/spark-apps/

# 4. Restart
make stream
```

### Backup Data
```powershell
# Cassandra
docker exec cassandra cqlsh -e "COPY hsbc.fraud_alerts TO '/tmp/backup.csv' WITH HEADER=TRUE;"
docker cp cassandra:/tmp/backup.csv ./backups/

# XGBoost Model
docker cp spark-master:/opt/data/models/fraud_xgb_21features ./backups/model_backup/
```

---

**üí° Tip**: Bookmark file n√†y ƒë·ªÉ reference nhanh c√°c l·ªánh th∆∞·ªùng d√πng!

**üìö Xem th√™m**:
- [Complete System Guide](SYSTEM_GUIDE.md) - H∆∞·ªõng d·∫´n chi ti·∫øt
- [Quick Start](QUICKSTART.md) - Kh·ªüi ƒë·ªông nhanh 5 ph√∫t
- [README](README.md) - T·ªïng quan h·ªá th·ªëng
