# ðŸ”„ HÆ°á»›ng Dáº«n Reset vÃ  Cháº¡y Láº¡i Streaming tá»« Äáº§u

## ðŸ“‹ TÃ³m táº¯t

Äá»ƒ reset vÃ  cháº¡y láº¡i streaming pipeline tá»« Ä‘áº§u (Ä‘á»c láº¡i táº¥t cáº£ messages tá»« Kafka), báº¡n cáº§n:
1. XÃ³a dá»¯ liá»‡u trong Cassandra
2. Cáº¥u hÃ¬nh Kafka offset vá» "earliest"
3. Restart streaming pipeline

## ðŸš€ CÃ¡c bÆ°á»›c thá»±c hiá»‡n

### **BÆ°á»›c 1: Stop cÃ¡c services (giá»¯ infrastructure)**

```powershell
# Stop producer vÃ  streaming (giá»¯ Kafka, Cassandra, Spark infrastructure)
docker compose stop producer
docker exec spark-master pkill -f "unified_streaming.py"
```

### **BÆ°á»›c 2: XÃ³a dá»¯ liá»‡u Cassandra**

```powershell
# Truncate báº£ng fraud_alerts Ä‘á»ƒ xÃ³a táº¥t cáº£ data
docker exec cassandra cqlsh -e "TRUNCATE hsbc.fraud_alerts;"

# Verify Ä‘Ã£ xÃ³a
docker exec cassandra cqlsh -e "SELECT COUNT(*) FROM hsbc.fraud_alerts;"
# Output: count = 0 âœ…
```

### **BÆ°á»›c 3: Reset Kafka Topic (Optional)**

**Option A: XÃ³a vÃ  táº¡o láº¡i topic (recommended cho reset hoÃ n toÃ n)**

```powershell
# Delete topic
docker exec kafka kafka-topics --bootstrap-server localhost:9092 --delete --topic transactions

# Recreate topic
docker exec kafka kafka-topics --create --topic transactions --bootstrap-server localhost:9092 --partitions 3 --replication-factor 1
```

**Option B: Giá»¯ nguyÃªn topic** (streaming sáº½ Ä‘á»c láº¡i tá»« offset earliest)

Náº¿u giá»¯ nguyÃªn topic, messages cÅ© váº«n cÃ²n trong Kafka retention period (default 7 days).

### **BÆ°á»›c 4: Cáº¥u hÃ¬nh Streaming Ä‘á»c tá»« Ä‘áº§u**

File: `streaming-pipeline/unified_streaming.py`

```python
# BEFORE (chá»‰ Ä‘á»c message má»›i):
.option("startingOffsets", "latest")

# AFTER (Ä‘á»c táº¥t cáº£ messages tá»« Ä‘áº§u):
.option("startingOffsets", "earliest")
```

**LÆ°u Ã½ quan trá»ng:**
- `"latest"`: Chá»‰ Ä‘á»c messages Má»šI sau khi streaming start
- `"earliest"`: Äá»c Táº¤T Cáº¢ messages tá»« Ä‘áº§u topic (hoáº·c tá»« retention period)

### **BÆ°á»›c 5: Install dependencies (náº¿u chÆ°a cÃ³)**

```powershell
# Spark-master container cáº§n cÃ¡c packages nÃ y
docker exec spark-master pip3 install numpy pandas xgboost scikit-learn pyarrow
```

### **BÆ°á»›c 6: Copy code má»›i vÃ o container**

```powershell
# Copy updated streaming script
docker cp streaming-pipeline/unified_streaming.py spark-master:/opt/spark-apps/
```

### **BÆ°á»›c 7: Restart Producer**

```powershell
# Restart producer Ä‘á»ƒ gá»­i láº¡i data tá»« Ä‘áº§u
docker compose restart producer

# Check producer logs
docker logs producer --tail 5
```

### **BÆ°á»›c 8: Start Streaming Pipeline**

```powershell
docker exec -d spark-master bash -c "cd /opt/spark-apps && export PYSPARK_PYTHON=/usr/bin/python3 ; /opt/spark/bin/spark-submit --master local[4] --driver-memory 2g --conf spark.sql.shuffle.partitions=20 --conf spark.streaming.kafka.consumer.poll.ms=256 --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,org.apache.hadoop:hadoop-aws:3.3.4,com.datastax.spark:spark-cassandra-connector_2.12:3.4.0 /opt/spark-apps/unified_streaming.py"
```

### **BÆ°á»›c 9: Verify Streaming hoáº¡t Ä‘á»™ng**

```powershell
# Wait 20-30 seconds
Start-Sleep -Seconds 25

# Check fraud alerts count (should be > 0 and increasing)
docker exec cassandra cqlsh -e "SELECT COUNT(*) FROM hsbc.fraud_alerts;"

# Check producer progress
docker logs producer --tail 3

# Check streaming process
docker exec spark-master ps aux | Select-String "spark-submit"
```

### **BÆ°á»›c 10: Start Dashboard (Optional)**

```powershell
docker compose up -d api dashboard

# Access dashboard
# http://localhost:8501
```

---

## ðŸ”§ Script Tá»± Äá»™ng Reset

Táº¡o file `reset_streaming.ps1`:

```powershell
# reset_streaming.ps1
Write-Host "ðŸ”„ Resetting Streaming Pipeline..." -ForegroundColor Cyan

# 1. Stop services
Write-Host "1ï¸âƒ£ Stopping producer and streaming..." -ForegroundColor Yellow
docker compose stop producer
docker exec spark-master pkill -f "unified_streaming.py" 2>$null

# 2. Truncate Cassandra
Write-Host "2ï¸âƒ£ Truncating Cassandra fraud_alerts table..." -ForegroundColor Yellow
docker exec cassandra cqlsh -e "TRUNCATE hsbc.fraud_alerts;"

# 3. Verify
$count = docker exec cassandra cqlsh -e "SELECT COUNT(*) FROM hsbc.fraud_alerts;" | Select-String "\d+" | ForEach-Object { $_.Matches.Value }
Write-Host "   Cassandra count: $count (should be 0)" -ForegroundColor Green

# 4. Delete and recreate Kafka topic
Write-Host "3ï¸âƒ£ Resetting Kafka topic..." -ForegroundColor Yellow
docker exec kafka kafka-topics --bootstrap-server localhost:9092 --delete --topic transactions 2>$null
Start-Sleep -Seconds 2
docker exec kafka kafka-topics --create --topic transactions --bootstrap-server localhost:9092 --partitions 3 --replication-factor 1

# 5. Copy updated streaming code
Write-Host "4ï¸âƒ£ Copying updated streaming code..." -ForegroundColor Yellow
docker cp streaming-pipeline/unified_streaming.py spark-master:/opt/spark-apps/

# 6. Restart producer
Write-Host "5ï¸âƒ£ Restarting producer..." -ForegroundColor Yellow
docker compose restart producer
Start-Sleep -Seconds 5

# 7. Start streaming
Write-Host "6ï¸âƒ£ Starting streaming pipeline..." -ForegroundColor Yellow
docker exec -d spark-master bash -c "cd /opt/spark-apps && export PYSPARK_PYTHON=/usr/bin/python3 ; /opt/spark/bin/spark-submit --master local[4] --driver-memory 2g --conf spark.sql.shuffle.partitions=20 --conf spark.streaming.kafka.consumer.poll.ms=256 --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,org.apache.hadoop:hadoop-aws:3.3.4,com.datastax.spark:spark-cassandra-connector_2.12:3.4.0 /opt/spark-apps/unified_streaming.py"

# 8. Wait and verify
Write-Host "7ï¸âƒ£ Waiting for streaming to process..." -ForegroundColor Yellow
Start-Sleep -Seconds 25

$newCount = docker exec cassandra cqlsh -e "SELECT COUNT(*) FROM hsbc.fraud_alerts;" | Select-String "\d+" | ForEach-Object { $_.Matches.Value }
Write-Host "   New fraud alerts: $newCount" -ForegroundColor Green

Write-Host "âœ… Reset complete! Streaming is processing from beginning." -ForegroundColor Green
Write-Host ""
Write-Host "ðŸ“Š Monitor progress:" -ForegroundColor Cyan
Write-Host "   Producer:  docker logs producer --tail 5" -ForegroundColor White
Write-Host "   Cassandra: docker exec cassandra cqlsh -e 'SELECT COUNT(*) FROM hsbc.fraud_alerts;'" -ForegroundColor White
```

**Cháº¡y script:**

```powershell
.\reset_streaming.ps1
```

---

## ðŸŽ¯ Káº¿t quáº£ Mong Äá»£i

### **Sau khi reset thÃ nh cÃ´ng:**

```
Producer:
- Sent: 3,900 transactions
- Fraud: 405 actual fraud (10.38%)

Cassandra:
- Fraud alerts: 439 (CHá»ˆ fraud detected)

Detection:
- Model Ä‘ang xá»­ lÃ½ tá»« earliest offset
- Alerts tÄƒng dáº§n theo producer progress
```

### **Timeline:**

| Thá»i gian | Producer Sent | Actual Fraud | Cassandra Alerts |
|-----------|---------------|--------------|------------------|
| 0s        | 0             | 0            | 0 (truncated)    |
| 30s       | 500           | 50           | 52-55            |
| 60s       | 1,000         | 100          | 105-110          |
| 5 min     | 3,900         | 405          | 430-450          |
| 10 min    | 7,800         | 810          | 850-900          |

---

## âš ï¸ Troubleshooting

### **Problem 1: Cassandra count váº«n lÃ  0**

**NguyÃªn nhÃ¢n:**
- Streaming process chÆ°a start hoáº·c bá»‹ lá»—i
- Producer chÆ°a gá»­i data
- Kafka topic trá»‘ng

**Giáº£i phÃ¡p:**

```powershell
# Check streaming process
docker exec spark-master ps aux | Select-String "spark-submit"

# Check producer
docker logs producer --tail 5

# Check Kafka messages
docker exec kafka kafka-run-class kafka.tools.GetOffsetShell --broker-list localhost:9092 --topic transactions --time -1
```

### **Problem 2: ModuleNotFoundError: No module named 'numpy'**

**NguyÃªn nhÃ¢n:**
- Spark-master container thiáº¿u Python dependencies

**Giáº£i phÃ¡p:**

```powershell
docker exec spark-master pip3 install numpy pandas xgboost scikit-learn pyarrow
```

### **Problem 3: Streaming Ä‘á»c tá»« "latest" thay vÃ¬ "earliest"**

**NguyÃªn nhÃ¢n:**
- File `unified_streaming.py` váº«n cÃ³ `startingOffsets="latest"`

**Giáº£i phÃ¡p:**

```python
# Edit streaming-pipeline/unified_streaming.py line 128
.option("startingOffsets", "earliest")  # â† Change from "latest"

# Copy to container
docker cp streaming-pipeline/unified_streaming.py spark-master:/opt/spark-apps/

# Restart streaming
docker exec spark-master pkill -f "unified_streaming.py"
# Then start again
```

### **Problem 4: Fraud alerts khÃ´ng tÄƒng**

**Check points:**

```powershell
# 1. Streaming process running?
docker exec spark-master ps aux | Select-String "spark"

# 2. Producer sending?
docker logs producer --tail 5

# 3. Kafka has messages?
docker exec kafka kafka-run-class kafka.tools.GetOffsetShell --broker-list localhost:9092 --topic transactions --time -1

# 4. Model loaded?
docker logs spark-master 2>&1 | Select-String "XGBoost|Model"
```

---

## ðŸ“š Giáº£i thÃ­ch Ká»¹ thuáº­t

### **Kafka Offset Management**

```
startingOffsets="earliest":
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Kafka Topic: transactions       â”‚
â”‚                                 â”‚
â”‚ [Offset 0] â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’   â”‚ â† Äá»c tá»« Ä‘Ã¢y
â”‚ [Offset 1]                      â”‚
â”‚ [Offset 2]                      â”‚
â”‚ ...                             â”‚
â”‚ [Offset 3,900] (latest)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

startingOffsets="latest":
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Kafka Topic: transactions       â”‚
â”‚                                 â”‚
â”‚ [Offset 0]                      â”‚
â”‚ [Offset 1]                      â”‚
â”‚ [Offset 2]                      â”‚
â”‚ ...                             â”‚
â”‚ [Offset 3,900] â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’   â”‚ â† Äá»c tá»« Ä‘Ã¢y (bá» qua cÅ©)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **Data Flow**

```
Producer (df_test_hdfs.csv)
    â†“
    ðŸ“¤ Send to Kafka
    â†“
Kafka Topic (transactions)
    â†“ startingOffsets="earliest" â† Äá»c tá»« Ä‘áº§u
    â†“
Spark Streaming
    â”œâ”€ Feature Engineering
    â”œâ”€ XGBoost Prediction
    â””â”€ Filter (prediction=1)
         â†“
    Cassandra (fraud_alerts) â† CHá»ˆ fraud detected
```

---

## âœ… Best Practices

1. **Backup trÆ°á»›c khi reset:**
   ```powershell
   # Backup Cassandra data
   docker exec cassandra cqlsh -e "COPY hsbc.fraud_alerts TO '/tmp/fraud_alerts_backup.csv' WITH HEADER=TRUE;"
   ```

2. **Monitor trong quÃ¡ trÃ¬nh reset:**
   ```powershell
   # Terminal 1: Producer logs
   docker logs producer -f

   # Terminal 2: Cassandra count
   while ($true) { docker exec cassandra cqlsh -e "SELECT COUNT(*) FROM hsbc.fraud_alerts;"; Start-Sleep 10 }
   ```

3. **Verify model loaded:**
   ```powershell
   # Check streaming logs for model loading
   docker logs spark-master 2>&1 | Select-String "Loading.*model|XGBoost"
   ```

4. **Use "earliest" cho development, "latest" cho production:**
   - Development: Test vá»›i full dataset â†’ `"earliest"`
   - Production: Real-time processing â†’ `"latest"`

---

**Last Updated**: 2025-11-20  
**Tested**: âœ… Working  
**Status**: Production Ready
