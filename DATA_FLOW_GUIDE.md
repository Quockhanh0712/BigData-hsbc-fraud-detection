# ğŸ“Š TÃ€I LIá»†U LUá»’NG Dá»® LIá»†U Há»† THá»NG HSBC FRAUD DETECTION

## ğŸ“‹ Má»¤C Lá»¤C
1. [Tá»•ng Quan Luá»“ng Dá»¯ Liá»‡u](#1-tá»•ng-quan-luá»“ng-dá»¯-liá»‡u)
2. [Chi Tiáº¿t Tá»«ng Giai Äoáº¡n](#2-chi-tiáº¿t-tá»«ng-giai-Ä‘oáº¡n)
3. [Dá»¯ Liá»‡u Training](#3-dá»¯-liá»‡u-training)
4. [Dá»¯ Liá»‡u Streaming](#4-dá»¯-liá»‡u-streaming)
5. [Schema Chi Tiáº¿t](#5-schema-chi-tiáº¿t)

---

## 1. Tá»”NG QUAN LUá»’NG Dá»® LIá»†U

### 1.1 Kiáº¿n TrÃºc Kappa - Luá»“ng Duy Nháº¥t

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         HSBC FRAUD DETECTION SYSTEM                      â”‚
â”‚                         Data Flow Architecture                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

GIAI ÄOáº N 1: DATA SOURCE (Nguá»“n Dá»¯ Liá»‡u)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“ /data/raw/
   â”œâ”€â”€ fraudTrain.csv      1,296,675 dÃ²ng (Training - 100%)
   â”œâ”€â”€ fraudTest.csv         555,719 dÃ²ng (Testing)
   â””â”€â”€ df_test_hdfs.csv      ~10% fraud rate (Production replay)

ThÃ´ng tin dataset:
â€¢ Tá»•ng cá»™t: 23 columns
â€¢ Fraud rate: 0.58% (fraudTrain), ~10% (df_test_hdfs)
â€¢ Format: CSV â†’ JSON â†’ Kafka Messages
â€¢ Size: ~500MB raw data

                              â†“

GIAI ÄOáº N 2: PRODUCER (PhÃ¡t Sinh Giao Dá»‹ch)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ”„ Producer Container (producer.py)

Nhiá»‡m vá»¥:
1. Äá»c CSV file (df_test_hdfs.csv)
2. Chuyá»ƒn Ä‘á»•i pandas DataFrame â†’ JSON
3. Shuffle data (random replay)
4. Gá»­i lÃªn Kafka vá»›i rate control

Xá»­ lÃ½:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Load CSV                         â”‚
â”‚    â€¢ Read with pandas               â”‚
â”‚    â€¢ Parse datetime fields          â”‚
â”‚    â€¢ Handle data types (cc_numâ†’str) â”‚
â”‚    â€¢ Shuffle for randomness         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2. Convert Row â†’ JSON               â”‚
â”‚    â€¢ 23 fields mapping              â”‚
â”‚    â€¢ Type conversion (int64â†’int)    â”‚
â”‚    â€¢ ISO timestamp format           â”‚
â”‚    â€¢ Preserve leading zeros (zip)   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 3. Send to Kafka                    â”‚
â”‚    â€¢ Topic: transactions_hsbc       â”‚
â”‚    â€¢ Key: transaction_id            â”‚
â”‚    â€¢ Rate: 2-12 tx/sec (config)     â”‚
â”‚    â€¢ Compression: gzip              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Output: JSON Messages â†’ Kafka Topic
Rate: ~12.5 transactions/second (configurable)

                              â†“

GIAI ÄOáº N 3: KAFKA (Message Queue)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“¨ Kafka Cluster

Topic Configuration:
â€¢ Topic: transactions_hsbc
â€¢ Partitions: 3
â€¢ Replication: 1
â€¢ Retention: 7 days

Message Format:
{
  "transaction_id": "abc123...",
  "transaction_time": "2024-01-15T10:30:45",
  "amount": 89.50,
  "merchant": "Target",
  "category": "grocery_pos",
  ... (23 fields total)
}

Storage: In-memory + disk (temporary)
Purpose: Decoupling Producer â†” Consumer

                              â†“

GIAI ÄOáº N 4: SPARK STREAMING (Xá»­ LÃ½ Real-time)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
âš¡ Spark Streaming Pipeline (unified_streaming.py)

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              SPARK STREAMING PROCESSING                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                          â”‚
â”‚  Step 1: READ FROM KAFKA                                â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                               â”‚
â”‚  â€¢ Subscribe topic: transactions_hsbc                   â”‚
â”‚  â€¢ Offset: earliest (Ä‘á»c tá»« Ä‘áº§u)                        â”‚
â”‚  â€¢ Batch: 1000 messages/trigger                         â”‚
â”‚  â€¢ Interval: 2 seconds                                  â”‚
â”‚                                                          â”‚
â”‚  Step 2: PARSE JSON                                     â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                    â”‚
â”‚  â€¢ from_json() vá»›i schema 23 cá»™t                        â”‚
â”‚  â€¢ Validate data types                                  â”‚
â”‚  â€¢ Add kafka_timestamp                                  â”‚
â”‚                                                          â”‚
â”‚  Step 3: FEATURE ENGINEERING (21 features)              â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                 â”‚
â”‚  ğŸ”§ Feature Engineer (feature_engineering.py)           â”‚
â”‚                                                          â”‚
â”‚  A. NUMERIC FEATURES (4)                                â”‚
â”‚     â€¢ amount            : Original transaction amount   â”‚
â”‚     â€¢ amount_log        : log1p(amount)                 â”‚
â”‚     â€¢ is_high_value     : 1 if amount > $100           â”‚
â”‚     â€¢ is_extreme_value  : 1 if amount > $500           â”‚
â”‚                                                          â”‚
â”‚  B. DEMOGRAPHIC FEATURES (2)                            â”‚
â”‚     â€¢ age               : Calculated from dob           â”‚
â”‚     â€¢ gender_encoded    : M=1, F=0, Other=-1           â”‚
â”‚                                                          â”‚
â”‚  C. TEMPORAL FEATURES (2)                               â”‚
â”‚     â€¢ hour_of_day       : 0-23 extracted from time     â”‚
â”‚     â€¢ is_weekend        : 1 if Sat/Sun                 â”‚
â”‚                                                          â”‚
â”‚  D. GEOGRAPHIC FEATURES (2)                             â”‚
â”‚     â€¢ distance_customer_merchant : Haversine distance  â”‚
â”‚     â€¢ is_out_of_state   : 1 if cross-state            â”‚
â”‚                                                          â”‚
â”‚  E. CATEGORY ONE-HOT (13)                               â”‚
â”‚     â€¢ cat_grocery_pos, cat_shopping_net,               â”‚
â”‚       cat_misc_net, cat_gas_transport,                 â”‚
â”‚       cat_shopping_pos, cat_food_dining,               â”‚
â”‚       cat_personal_care, cat_health_fitness,           â”‚
â”‚       cat_entertainment, cat_utilities,                â”‚
â”‚       cat_travel, cat_electronics, cat_others          â”‚
â”‚                                                          â”‚
â”‚  F. ADDITIONAL FEATURES (2)                             â”‚
â”‚     â€¢ amt_to_pop_ratio  : amount / city_pop            â”‚
â”‚     â€¢ city_pop          : Customer city population     â”‚
â”‚                                                          â”‚
â”‚  Total: 21 engineered features                          â”‚
â”‚                                                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                          â”‚
â”‚  Step 4: DUAL STREAM (Kappa Architecture)               â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€              â”‚
â”‚                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  STREAM A: ARCHIVE  â”‚   â”‚ STREAM B: INFERENCE â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚           â”‚                          â”‚                  â”‚
â”‚           â†“                          â†“                  â”‚
â”‚    [MinIO Storage]          [ML Model Prediction]      â”‚
â”‚                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                    â†“                        â†“
              
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   STREAM A DETAIL   â”‚        â”‚   STREAM B DETAIL   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

STREAM A: ARCHIVE (Data Lake)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“¦ MinIO (S3-compatible Storage)

Purpose: LÆ°u trá»¯ lá»‹ch sá»­ Ä‘á»ƒ retrain model

Process:
1. Láº¥y transactions + features
2. Partition by date
3. Write as Parquet format
4. Checkpoint for fault tolerance

Storage Path:
s3a://hsbc-data/
  â”œâ”€â”€ stream-archive/
  â”‚   â””â”€â”€ transactions/
  â”‚       â”œâ”€â”€ date=2024-01-15/
  â”‚       â”‚   â”œâ”€â”€ part-00000.parquet
  â”‚       â”‚   â””â”€â”€ part-00001.parquet
  â”‚       â””â”€â”€ date=2024-01-16/
  â””â”€â”€ checkpoints/
      â””â”€â”€ archive/

Configuration:
â€¢ Trigger: Every 10 seconds
â€¢ Format: Parquet (compressed)
â€¢ Partitioning: By date column
â€¢ Mode: Append

Data Retention: Permanent (for retraining)


STREAM B: INFERENCE (Real-time Detection)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ¤– ML Model Prediction

Purpose: PhÃ¡t hiá»‡n gian láº­n real-time

Process:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Load Model                      â”‚
â”‚    Path: /opt/data/models/         â”‚
â”‚          fraud_xgb_21features      â”‚
â”‚    Type: XGBoost Pipeline          â”‚
â”‚    Features: 21 inputs             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2. Batch Processing                â”‚
â”‚    â€¢ Trigger: Every 2 seconds      â”‚
â”‚    â€¢ Process micro-batch           â”‚
â”‚    â€¢ Apply model.transform()       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 3. Prediction                      â”‚
â”‚    Model Output:                   â”‚
â”‚    â€¢ prediction: 0 (normal)        â”‚
â”‚                  1 (FRAUD)         â”‚
â”‚    â€¢ probability: [p0, p1]         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 4. Filter Fraud Alerts             â”‚
â”‚    â€¢ Keep only prediction = 1      â”‚
â”‚    â€¢ Add detected_at timestamp     â”‚
â”‚    â€¢ Prepare for Cassandra         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Logging: Chi tiáº¿t má»—i fraud transaction
â€¢ Transaction ID, Time, Amount
â€¢ Merchant, Category
â€¢ Customer name, Location
â€¢ Card number (masked)

                    â†“

GIAI ÄOáº N 5: CASSANDRA (Fraud Alerts Storage)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ—„ï¸ Cassandra Database

Keyspace: hsbc
Table: fraud_alerts

**QUAN TRá»ŒNG**: Chá»‰ lÆ°u FRAUD DETECTED (prediction=1)
                KHÃ”NG lÆ°u táº¥t cáº£ transactions

Insert Process:
1. Receive fraud alerts from Spark
2. Write to Cassandra (append mode)
3. Primary key: transaction_id
4. Add detected_at timestamp

Storage Schema:
CREATE TABLE hsbc.fraud_alerts (
    transaction_id text PRIMARY KEY,
    transaction_time timestamp,
    amount double,
    merchant text,
    category text,
    cc_num text,
    first text,
    last text,
    gender text,
    job text,
    state text,
    city text,
    zip text,
    is_fraud double,
    detected_at timestamp
);

Query Pattern: 
â€¢ Real-time dashboard queries
â€¢ API endpoints
â€¢ Fraud analysis

Data Volume: ~0.6-10% of input (chá»‰ fraud)

                    â†“

GIAI ÄOáº N 6: API & DASHBOARD (Visualization)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸŒ FastAPI + ğŸ“Š Streamlit

API (FastAPI):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Endpoints:                      â”‚
â”‚ â€¢ GET /fraud/alerts             â”‚
â”‚ â€¢ GET /fraud/stats              â”‚
â”‚ â€¢ GET /fraud/recent             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Cassandra Connection:           â”‚
â”‚ â€¢ Host: cassandra:9042          â”‚
â”‚ â€¢ Keyspace: hsbc                â”‚
â”‚ â€¢ Query: SELECT * FROM alerts   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Dashboard (Streamlit):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Real-time Charts:               â”‚
â”‚ â€¢ Fraud count over time         â”‚
â”‚ â€¢ Amount distribution           â”‚
â”‚ â€¢ Category breakdown            â”‚
â”‚ â€¢ Geographic heatmap            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Data Source: API REST calls     â”‚
â”‚ Refresh: Every 3 seconds        â”‚
â”‚ Port: http://localhost:8501     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 2. CHI TIáº¾T Tá»ªNG GIAI ÄOáº N

### GIAI ÄOáº N 2.1: PRODUCER - Äá»c vÃ  Chuyá»ƒn Äá»•i Dá»¯ Liá»‡u

#### A. Load CSV File

```python
# File: producer/producer.py

def load_data(self):
    """
    Äá»c CSV vá»›i pandas
    Input: /data/raw/df_test_hdfs.csv
    Output: pandas DataFrame
    """
    
    # 1. Define data types Ä‘á»ƒ trÃ¡nh overflow
    dtype_spec = {
        'cc_num': str,        # Card number as string (16 digits)
        'zip': str,           # ZIP code with leading zeros
        'is_fraud': int,      # Target: 0 or 1
        'city_pop': int       # City population
    }
    
    # 2. Read CSV
    self.df = pd.read_csv(
        '/data/raw/df_test_hdfs.csv',
        dtype=dtype_spec,
        parse_dates=['trans_date_trans_time']  # Parse datetime
    )
    
    # 3. Data preprocessing
    self.df['hour_of_day'] = self.df['trans_date_trans_time'].dt.hour
    
    # 4. Shuffle for random replay
    self.df = self.df.sample(frac=1, random_state=42).reset_index(drop=True)
    
    # Statistics
    total = len(self.df)                    # ~100K rows
    fraud_count = self.df['is_fraud'].sum() # ~10K fraud
    fraud_rate = fraud_count / total        # ~10%
```

**Dá»¯ liá»‡u Ä‘áº§u vÃ o (CSV row example):**
```csv
trans_num,trans_date_trans_time,cc_num,merchant,category,amt,first,last,gender,street,city,state,zip,lat,long,city_pop,job,dob,trans_num,unix_time,merch_lat,merch_long,is_fraud
abc123,2024-01-15 10:30:45,4532123456789012,Target,grocery_pos,89.50,John,Smith,M,123 Main St,Springfield,IL,62701,39.7817,-89.6501,116250,Engineer,1985-03-15,123456789,1705315845,39.7850,-89.6450,0
```

**Dá»¯ liá»‡u Ä‘áº§u ra (JSON object):**
```json
{
  "transaction_id": "abc123",
  "transaction_time": "2024-01-15T10:30:45",
  "amount": 89.50,
  "unix_time": 1705315845,
  "hour_of_day": 10,
  "cc_num": "4532123456789012",
  "first": "John",
  "last": "Smith",
  "gender": "M",
  "dob": "1985-03-15",
  "job": "Engineer",
  "street": "123 Main St",
  "city": "Springfield",
  "state": "IL",
  "zip": "62701",
  "lat": 39.7817,
  "long": -89.6501,
  "city_pop": 116250,
  "merchant": "Target",
  "category": "grocery_pos",
  "merch_lat": 39.7850,
  "merch_long": -89.6450,
  "is_fraud": 0
}
```

#### B. Send to Kafka

```python
def send_transaction(self, transaction):
    """
    Gá»­i JSON lÃªn Kafka
    """
    self.producer.send(
        topic='transactions_hsbc',
        value=transaction,                          # JSON object
        key=transaction['transaction_id'].encode() # Partition key
    )
```

**Kafka Message Structure:**
```
Key: "abc123" (transaction_id)
Value: {JSON object 23 fields}
Topic: transactions_hsbc
Partition: Auto (based on key hash)
Timestamp: Kafka ingestion time
```

**Rate Control:**
- Config: `TRANSACTION_RATE = 12` (transactions/second)
- Sleep: `time.sleep(1 / 12)` = 0.083 seconds between sends
- Daily volume: 12 * 60 * 60 * 24 = 1,036,800 transactions/day

---

### GIAI ÄOáº N 4.1: SPARK STREAMING - Feature Engineering Chi Tiáº¿t

#### Input: Raw Transaction (23 fields)

```python
# Original columns tá»« Kafka
raw_transaction = {
    "transaction_id": "abc123",
    "transaction_time": "2024-01-15T10:30:45",
    "amount": 89.50,
    "unix_time": 1705315845,
    "hour_of_day": 10,
    "cc_num": "4532123456789012",
    "first": "John",
    "last": "Smith",
    "gender": "M",
    "dob": "1985-03-15",
    "job": "Engineer",
    "street": "123 Main St",
    "city": "Springfield",
    "state": "IL",
    "zip": "62701",
    "lat": 39.7817,
    "long": -89.6501,
    "city_pop": 116250,
    "merchant": "Target",
    "category": "grocery_pos",
    "merch_lat": 39.7850,
    "merch_long": -89.6450,
    "is_fraud": 0
}
```

#### Feature Engineering Process (feature_engineering.py)

```python
class FeatureEngineer:
    def engineer_features(self, df):
        """
        Transform 23 raw columns â†’ 44 columns (23 original + 21 features)
        """
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # 1. NUMERIC FEATURES (4 features)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        
        # F1: amount_log - Log transformation for skewed distribution
        df = df.withColumn('amount_log', log1p(col('amount')))
        # Example: amount=89.50 â†’ amount_log=4.50
        
        # F2: is_high_value - Flag for high-value transactions
        df = df.withColumn('is_high_value', 
                          when(col('amount') > 100, 1).otherwise(0))
        # Example: 89.50 < 100 â†’ is_high_value=0
        
        # F3: is_extreme_value - Flag for very high amounts
        df = df.withColumn('is_extreme_value', 
                          when(col('amount') > 500, 1).otherwise(0))
        # Example: 89.50 < 500 â†’ is_extreme_value=0
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # 2. DEMOGRAPHIC FEATURES (2 features)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        
        # F4: age - Calculate from date of birth
        df = df.withColumn('age', 
            floor(months_between(current_date(), to_date(col('dob'))) / 12))
        # Example: dob="1985-03-15" â†’ age=39 (in 2024)
        
        # F5: gender_encoded - Numerical encoding
        df = df.withColumn('gender_encoded',
            when(lower(col('gender')) == 'm', 1)
            .when(lower(col('gender')) == 'f', 0)
            .otherwise(-1))
        # Example: gender="M" â†’ gender_encoded=1
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # 3. TEMPORAL FEATURES (2 features)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        
        # F6: hour_of_day - Already provided, validate
        df = df.withColumn('hour_of_day', hour(col('transaction_time')))
        # Example: "10:30:45" â†’ hour_of_day=10
        
        # F7: is_weekend - Weekend indicator
        df = df.withColumn('is_weekend', 
            when(dayofweek(col('transaction_time')).isin([1, 7]), 1)
            .otherwise(0))
        # Example: Monday â†’ is_weekend=0
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # 4. GEOGRAPHIC FEATURES (2 features)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        
        # F8: distance_customer_merchant - Haversine formula
        df = df.withColumn('distance_customer_merchant',
            2 * 6371 * asin(sqrt(
                pow(sin(radians(col('merch_lat') - col('lat')) / 2), 2) +
                cos(radians(col('lat'))) * 
                cos(radians(col('merch_lat'))) * 
                pow(sin(radians(col('merch_long') - col('long')) / 2), 2)
            )))
        # Example: Customer(39.7817,-89.6501) â†’ Merchant(39.7850,-89.6450)
        #          distance_customer_merchant = 0.58 km
        
        # F9: is_out_of_state - Cross-state transaction
        df = df.withColumn('is_out_of_state', lit(0))
        # Currently disabled (merchant_state not available)
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # 5. CATEGORY ONE-HOT ENCODING (13 features)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        
        categories = [
            'grocery_pos',      # F10
            'shopping_net',     # F11
            'misc_net',         # F12
            'gas_transport',    # F13
            'shopping_pos',     # F14
            'food_dining',      # F15
            'personal_care',    # F16
            'health_fitness',   # F17
            'entertainment',    # F18
            'utilities',        # F19
            'travel',           # F20
            'electronics',      # F21
            'others'            # F22
        ]
        
        for cat in categories:
            df = df.withColumn(f'cat_{cat}',
                when(lower(col('category')) == cat, 1).otherwise(0))
        
        # Example: category="grocery_pos"
        #   â†’ cat_grocery_pos=1, all other cat_*=0
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # 6. ADDITIONAL CRAFTED FEATURE (1 feature)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        
        # F23: amt_to_pop_ratio - Amount relative to city size
        df = df.withColumn('amt_to_pop_ratio',
            when((col('city_pop') > 0), 
                 col('amount') / col('city_pop'))
            .otherwise(0.0))
        # Example: amount=89.50, city_pop=116,250
        #          amt_to_pop_ratio = 0.00077
        
        return df
```

#### Output: Engineered Features (44 columns total)

```python
engineered_transaction = {
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ORIGINAL 23 FIELDS (kept)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    "transaction_id": "abc123",
    "transaction_time": "2024-01-15T10:30:45",
    "amount": 89.50,
    "unix_time": 1705315845,
    "hour_of_day": 10,
    "cc_num": "4532123456789012",
    "first": "John",
    "last": "Smith",
    "gender": "M",
    "dob": "1985-03-15",
    "job": "Engineer",
    "street": "123 Main St",
    "city": "Springfield",
    "state": "IL",
    "zip": "62701",
    "lat": 39.7817,
    "long": -89.6501,
    "city_pop": 116250,
    "merchant": "Target",
    "category": "grocery_pos",
    "merch_lat": 39.7850,
    "merch_long": -89.6450,
    "is_fraud": 0,
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ENGINEERED 21 FEATURES (new)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    # Numeric (4)
    "amount_log": 4.50,
    "is_high_value": 0,
    "is_extreme_value": 0,
    "amt_to_pop_ratio": 0.00077,
    
    # Demographic (2)
    "age": 39,
    "gender_encoded": 1,
    
    # Temporal (2)
    "is_weekend": 0,
    
    # Geographic (2)
    "distance_customer_merchant": 0.58,
    "is_out_of_state": 0,
    
    # Category one-hot (13)
    "cat_grocery_pos": 1,
    "cat_shopping_net": 0,
    "cat_misc_net": 0,
    "cat_gas_transport": 0,
    "cat_shopping_pos": 0,
    "cat_food_dining": 0,
    "cat_personal_care": 0,
    "cat_health_fitness": 0,
    "cat_entertainment": 0,
    "cat_utilities": 0,
    "cat_travel": 0,
    "cat_electronics": 0,
    "cat_others": 0
}
```

**Total Columns**: 23 original + 21 engineered = 44 columns

---

### GIAI ÄOáº N 4.2: MODEL INFERENCE - Dá»± ÄoÃ¡n Gian Láº­n

#### A. Load Model

```python
# Model: /opt/data/models/fraud_xgb_21features

model = PipelineModel.load('/opt/data/models/fraud_xgb_21features')

# Model Pipeline:
# 1. StringIndexer: is_fraud â†’ label
# 2. VectorAssembler: 21 features â†’ features vector
# 3. XGBoost Classifier: features â†’ prediction, probability
```

#### B. Feature Vector Preparation

```python
# Select 21 features for model input
feature_cols = [
    'amount', 'age', 'city_pop', 'hour_of_day', 'is_weekend',
    'amount_log', 'is_high_value', 'is_extreme_value',
    'distance_customer_merchant', 'is_out_of_state',
    'amt_to_pop_ratio', 'gender_encoded',
    'cat_grocery_pos', 'cat_shopping_net', 'cat_misc_net',
    'cat_gas_transport', 'cat_shopping_pos', 'cat_food_dining',
    'cat_personal_care', 'cat_health_fitness', 'cat_entertainment'
]

# VectorAssembler combines into single vector
features = [89.50, 39, 116250, 10, 0, 4.50, 0, 0, 0.58, 0, 
            0.00077, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]

# Dense vector format: [21 values]
```

#### C. XGBoost Prediction

```python
# Model.transform() applies prediction
predictions = model.transform(batch_df)

# Output columns:
# - prediction: 0 (normal) or 1 (fraud)
# - probability: [p_normal, p_fraud]
```

**Example Prediction Output:**
```python
{
    # Original + Features (44 cols)
    "transaction_id": "abc123",
    "amount": 89.50,
    ... (all previous fields),
    
    # Model Output (3 new cols)
    "features": DenseVector([89.50, 39, 116250, ...]),
    "probability": [0.92, 0.08],  # [normal, fraud]
    "prediction": 0.0              # 0=normal, 1=fraud
}
```

#### D. Filter Fraud Alerts

```python
# Keep only FRAUD predictions
fraud_alerts = predictions.filter(col("prediction") == 1)

# Select columns for Cassandra
fraud_alerts = fraud_alerts.select(
    col("transaction_id"),
    col("transaction_time"),
    col("amount"),
    col("merchant"),
    col("category"),
    col("cc_num"),
    col("first"),
    col("last"),
    col("gender"),
    col("job"),
    col("state"),
    col("city"),
    col("zip"),
    col("prediction").alias("is_fraud"),  # Rename: 1
    current_timestamp().alias("detected_at")
)
```

**Example Fraud Alert:**
```python
{
    "transaction_id": "xyz789",
    "transaction_time": "2024-01-15T23:45:30",
    "amount": 987.50,
    "merchant": "Online Store XYZ",
    "category": "shopping_net",
    "cc_num": "4532987654321098",
    "first": "Jane",
    "last": "Doe",
    "gender": "F",
    "job": "Manager",
    "state": "CA",
    "city": "Los Angeles",
    "zip": "90001",
    "is_fraud": 1.0,
    "detected_at": "2024-01-15T23:45:32"
}
```

---

### GIAI ÄOáº N 5.1: CASSANDRA - Chi Tiáº¿t LÆ°u Trá»¯

#### Table Schema

```cql
CREATE TABLE hsbc.fraud_alerts (
    -- Primary Key
    transaction_id text PRIMARY KEY,
    
    -- Transaction Info
    transaction_time timestamp,
    amount double,
    merchant text,
    category text,
    
    -- Customer Info
    cc_num text,
    first text,
    last text,
    gender text,
    job text,
    
    -- Location
    state text,
    city text,
    zip text,
    
    -- Detection Info
    is_fraud double,      -- Always 1.0 (fraud detected)
    detected_at timestamp -- When system detected
);
```

#### Insert Process

```python
# Spark writes directly to Cassandra
fraud_alerts.write \
    .format("org.apache.spark.sql.cassandra") \
    .mode("append") \
    .options(table="fraud_alerts", keyspace="hsbc") \
    .save()
```

**Data Flow:**
```
Spark DataFrame (fraud_alerts)
         â†“
Cassandra Connector
         â†“
hsbc.fraud_alerts table
         â†“
INSERT INTO hsbc.fraud_alerts 
VALUES ('xyz789', '2024-01-15 23:45:30', 987.50, ...)
```

#### Storage Characteristics

**Quan trá»ng - Hiá»ƒu Ä‘Ãºng vá» Cassandra:**

```
Producer gá»­i:     100,000 transactions
                      â†“
Spark xá»­ lÃ½:      100,000 transactions
                      â†“
Model dá»± Ä‘oÃ¡n:     10,000 fraud (10%)
                      â†“
Cassandra lÆ°u:     10,000 fraud ONLY âœ“

KHÃ”NG LÆ¯U 90,000 normal transactions!
```

**Tá»· lá»‡:**
- Input: 100% transactions
- Cassandra: ~0.6-10% (chá»‰ fraud detected)
- Compression ratio: 10-167x reduction

---

## 3. Dá»® LIá»†U TRAINING

### 3.1 Dataset Training (fraudTrain.csv)

**File:** `/data/raw/fraudTrain.csv`

**ThÃ´ng sá»‘:**
```
Rows:        1,296,675 transactions
Fraud:       7,506 (0.58%)
Normal:      1,289,169 (99.42%)
Columns:     23
Size:        ~300 MB
Format:      CSV with header
```

**Schema (23 columns):**
```python
fraudTrain.csv columns:
[
    'trans_num',              # Transaction ID
    'trans_date_trans_time',  # DateTime: YYYY-MM-DD HH:MM:SS
    'cc_num',                 # Credit card number (16 digits)
    'merchant',               # Merchant name
    'category',               # Transaction category
    'amt',                    # Amount ($)
    'first',                  # First name
    'last',                   # Last name
    'gender',                 # M/F
    'street',                 # Street address
    'city',                   # City
    'state',                  # State (2-letter code)
    'zip',                    # ZIP code (5 digits)
    'lat',                    # Customer latitude
    'long',                   # Customer longitude
    'city_pop',               # City population
    'job',                    # Job title
    'dob',                    # Date of birth
    'trans_num',              # (duplicate)
    'unix_time',              # Unix timestamp
    'merch_lat',              # Merchant latitude
    'merch_long',             # Merchant longitude
    'is_fraud'                # Target: 0 or 1
]
```

### 3.2 Training Process (model_retraining_xgb.py)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              MODEL TRAINING WORKFLOW                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Step 1: LOAD DATA
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Input:  /opt/data/raw/fraudTrain.csv
Method: spark.read.csv()
Result: 1,296,675 rows Ã— 23 columns

Step 2: FEATURE ENGINEERING
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Apply: feature_engineering.engineer_features()
Input:  23 raw columns
Output: 44 columns (23 + 21 features)

Step 3: FEATURE SELECTION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Select 21 features for model:
â€¢ amount, age, city_pop, hour_of_day, is_weekend
â€¢ amount_log, is_high_value, is_extreme_value
â€¢ distance_customer_merchant, is_out_of_state
â€¢ amt_to_pop_ratio, gender_encoded
â€¢ cat_grocery_pos, cat_shopping_net, cat_misc_net
â€¢ cat_gas_transport, cat_shopping_pos, cat_food_dining
â€¢ cat_personal_care, cat_health_fitness, cat_entertainment

Step 4: PREPARE LABEL
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
StringIndexer: is_fraud (0/1) â†’ label (0.0/1.0)

Step 5: VECTOR ASSEMBLY
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
VectorAssembler: 21 features â†’ Dense vector [21 values]

Step 6: TRAIN/TEST SPLIT
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Training set: 80% = 1,037,340 rows
Test set:     20% =   259,335 rows

Step 7: XGBOOST TRAINING
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Algorithm: XGBoost Classifier
Hyperparameters:
  â€¢ max_depth: 6
  â€¢ learning_rate: 0.3
  â€¢ n_estimators: 100 trees
  â€¢ subsample: 0.8
  â€¢ colsample_bytree: 0.8
  â€¢ eval_metric: AUC

Training time: ~6-10 minutes
Iterations: 100 boosting rounds

Step 8: EVALUATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Metric: AUC-ROC
Test set predictions:
  â€¢ True Positives
  â€¢ False Positives
  â€¢ True Negatives
  â€¢ False Negatives

Result: AUC-ROC = 0.9964 (99.64%)

Step 9: SAVE MODEL
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Path: /opt/data/models/fraud_xgb_21features/
Format: PipelineModel (Spark ML)
Components:
  â”œâ”€â”€ metadata/
  â”œâ”€â”€ stages/
  â”‚   â”œâ”€â”€ 0_StringIndexer/
  â”‚   â”œâ”€â”€ 1_VectorAssembler/
  â”‚   â””â”€â”€ 2_XGBoostClassifier/
  â””â”€â”€ _SUCCESS
```

### 3.3 Model Performance

**Confusion Matrix (Example):**
```
                 Predicted
                Fraud   Normal
Actual Fraud    [ TP ]  [ FN ]
       Normal   [ FP ]  [ TN ]

TP (True Positive):  ~7,400 (correctly detected fraud)
FN (False Negative): ~100   (missed fraud)
FP (False Positive): ~1,000 (false alarms)
TN (True Negative):  ~250,835 (correctly normal)

Precision: TP / (TP + FP) = 7,400 / 8,400 = 88%
Recall:    TP / (TP + FN) = 7,400 / 7,500 = 99%
F1-Score:  2 * (P * R) / (P + R) = 93%
AUC-ROC:   0.9964 (excellent)
```

---

## 4. Dá»® LIá»†U STREAMING

### 4.1 Production Dataset (df_test_hdfs.csv)

**File:** `/data/raw/df_test_hdfs.csv`

**Characteristics:**
```
Purpose:     Production replay for testing
Rows:        ~100,000 transactions
Fraud rate:  ~10% (higher than training for demo)
Format:      Same 23 columns as fraudTrain
Usage:       Producer reads and streams to Kafka
```

**Why different fraud rate?**
- Training: 0.58% (realistic fraud rate)
- Testing: 10% (easier to demo and monitor)

### 4.2 Streaming Data Volume

**Daily Throughput (vá»›i rate=12 tx/sec):**
```
Transactions per second:  12
Transactions per minute:  720
Transactions per hour:    43,200
Transactions per day:     1,036,800

Fraud detected (10%):     103,680 per day
Cassandra inserts:        103,680 per day
MinIO archive:            1,036,800 per day (all)
```

**Data Size Estimation:**
```
JSON message size:        ~1 KB per transaction
Kafka throughput:         12 KB/sec = 720 KB/min
Daily Kafka volume:       ~1 GB/day

Parquet (MinIO):          Compressed ~300 MB/day
Cassandra (fraud only):   ~100 MB/day
```

### 4.3 Data Retention

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Component       â”‚ Retention    â”‚ Purpose             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Kafka           â”‚ 7 days       â”‚ Message queue       â”‚
â”‚ MinIO Archive   â”‚ Permanent    â”‚ Model retraining    â”‚
â”‚ Cassandra       â”‚ Permanent    â”‚ Fraud analysis      â”‚
â”‚ Dashboard       â”‚ Real-time    â”‚ Visualization       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 5. SCHEMA CHI TIáº¾T

### 5.1 Complete Data Schema

```python
COMPLETE_SCHEMA = {
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # SECTION 1: IDENTIFIERS (2 fields)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    "transaction_id": {
        "type": "string",
        "format": "UUID or alphanumeric",
        "example": "abc123def456",
        "nullable": False,
        "source": "trans_num column"
    },
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # SECTION 2: TRANSACTION DETAILS (5 fields)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    "transaction_time": {
        "type": "timestamp",
        "format": "ISO 8601 (YYYY-MM-DDTHH:MM:SS)",
        "example": "2024-01-15T10:30:45",
        "nullable": False,
        "source": "trans_date_trans_time"
    },
    "amount": {
        "type": "double",
        "range": "[0.01, 10000.00]",
        "unit": "USD",
        "example": 89.50,
        "nullable": False,
        "source": "amt column"
    },
    "unix_time": {
        "type": "long",
        "format": "Unix epoch seconds",
        "example": 1705315845,
        "nullable": False,
        "source": "unix_time column"
    },
    "hour_of_day": {
        "type": "integer",
        "range": "[0, 23]",
        "example": 10,
        "nullable": False,
        "source": "Extracted from transaction_time"
    },
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # SECTION 3: CARD & CUSTOMER (6 fields)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    "cc_num": {
        "type": "string",
        "format": "16-digit credit card number",
        "example": "4532123456789012",
        "nullable": False,
        "masked": "****9012 (in display)",
        "source": "cc_num column"
    },
    "first": {
        "type": "string",
        "example": "John",
        "nullable": True,
        "source": "first column"
    },
    "last": {
        "type": "string",
        "example": "Smith",
        "nullable": True,
        "source": "last column"
    },
    "gender": {
        "type": "string",
        "values": ["M", "F"],
        "example": "M",
        "nullable": True,
        "source": "gender column"
    },
    "dob": {
        "type": "string",
        "format": "YYYY-MM-DD",
        "example": "1985-03-15",
        "nullable": True,
        "source": "dob column"
    },
    "job": {
        "type": "string",
        "example": "Engineer",
        "nullable": True,
        "source": "job column"
    },
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # SECTION 4: CUSTOMER LOCATION (7 fields)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    "street": {
        "type": "string",
        "example": "123 Main St",
        "nullable": True,
        "source": "street column"
    },
    "city": {
        "type": "string",
        "example": "Springfield",
        "nullable": True,
        "source": "city column"
    },
    "state": {
        "type": "string",
        "format": "2-letter code",
        "example": "IL",
        "nullable": True,
        "source": "state column"
    },
    "zip": {
        "type": "string",
        "format": "5-digit ZIP code",
        "example": "62701",
        "nullable": True,
        "note": "Leading zeros preserved",
        "source": "zip column"
    },
    "lat": {
        "type": "double",
        "range": "[-90, 90]",
        "example": 39.7817,
        "unit": "degrees",
        "nullable": True,
        "source": "lat column"
    },
    "long": {
        "type": "double",
        "range": "[-180, 180]",
        "example": -89.6501,
        "unit": "degrees",
        "nullable": True,
        "source": "long column"
    },
    "city_pop": {
        "type": "integer",
        "example": 116250,
        "nullable": True,
        "source": "city_pop column"
    },
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # SECTION 5: MERCHANT (4 fields)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    "merchant": {
        "type": "string",
        "example": "Target",
        "nullable": False,
        "source": "merchant column"
    },
    "category": {
        "type": "string",
        "values": [
            "grocery_pos", "shopping_net", "misc_net",
            "gas_transport", "shopping_pos", "food_dining",
            "personal_care", "health_fitness", "entertainment",
            "utilities", "travel", "electronics", "others"
        ],
        "example": "grocery_pos",
        "nullable": False,
        "source": "category column"
    },
    "merch_lat": {
        "type": "double",
        "range": "[-90, 90]",
        "example": 39.7850,
        "nullable": True,
        "source": "merch_lat column"
    },
    "merch_long": {
        "type": "double",
        "range": "[-180, 180]",
        "example": -89.6450,
        "nullable": True,
        "source": "merch_long column"
    },
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # SECTION 6: TARGET (1 field)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    "is_fraud": {
        "type": "integer",
        "values": [0, 1],
        "example": 0,
        "nullable": True,
        "note": "Ground truth label",
        "source": "is_fraud column"
    },
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # SECTION 7: ENGINEERED FEATURES (21 fields)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    # Numeric Features (4)
    "amount_log": {
        "type": "double",
        "formula": "log1p(amount)",
        "example": 4.50,
        "purpose": "Handle skewed distribution"
    },
    "is_high_value": {
        "type": "integer",
        "values": [0, 1],
        "formula": "amount > 100",
        "example": 0,
        "purpose": "Flag high-value transactions"
    },
    "is_extreme_value": {
        "type": "integer",
        "values": [0, 1],
        "formula": "amount > 500",
        "example": 0,
        "purpose": "Flag very high amounts"
    },
    "amt_to_pop_ratio": {
        "type": "double",
        "formula": "amount / city_pop",
        "example": 0.00077,
        "purpose": "Amount relative to city size"
    },
    
    # Demographic Features (2)
    "age": {
        "type": "integer",
        "formula": "floor(months_between(current_date, dob) / 12)",
        "example": 39,
        "purpose": "Customer age"
    },
    "gender_encoded": {
        "type": "integer",
        "values": [-1, 0, 1],
        "mapping": {"M": 1, "F": 0, "Other": -1},
        "example": 1,
        "purpose": "Numerical gender encoding"
    },
    
    # Temporal Features (2)
    "is_weekend": {
        "type": "integer",
        "values": [0, 1],
        "formula": "dayofweek in [1, 7]",
        "example": 0,
        "purpose": "Weekend indicator"
    },
    
    # Geographic Features (2)
    "distance_customer_merchant": {
        "type": "double",
        "unit": "kilometers",
        "formula": "haversine(customer_lat/long, merchant_lat/long)",
        "example": 0.58,
        "purpose": "Distance between customer and merchant"
    },
    "is_out_of_state": {
        "type": "integer",
        "values": [0, 1],
        "example": 0,
        "note": "Currently disabled",
        "purpose": "Cross-state transaction flag"
    },
    
    # Category One-Hot (13 features)
    "cat_grocery_pos": {"type": "integer", "values": [0, 1]},
    "cat_shopping_net": {"type": "integer", "values": [0, 1]},
    "cat_misc_net": {"type": "integer", "values": [0, 1]},
    "cat_gas_transport": {"type": "integer", "values": [0, 1]},
    "cat_shopping_pos": {"type": "integer", "values": [0, 1]},
    "cat_food_dining": {"type": "integer", "values": [0, 1]},
    "cat_personal_care": {"type": "integer", "values": [0, 1]},
    "cat_health_fitness": {"type": "integer", "values": [0, 1]},
    "cat_entertainment": {"type": "integer", "values": [0, 1]},
    "cat_utilities": {"type": "integer", "values": [0, 1]},
    "cat_travel": {"type": "integer", "values": [0, 1]},
    "cat_electronics": {"type": "integer", "values": [0, 1]},
    "cat_others": {"type": "integer", "values": [0, 1]}
}

# Total: 23 original + 21 engineered = 44 columns
```

### 5.2 Data Type Mapping

```
CSV (pandas)  â†’  Kafka (JSON)  â†’  Spark  â†’  Cassandra
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
str           â†’  string        â†’  string â†’  text
datetime64    â†’  string (ISO)  â†’  timestamp â†’ timestamp
int64         â†’  number        â†’  long   â†’  bigint
float64       â†’  number        â†’  double â†’  double
object        â†’  string        â†’  string â†’  text
```

---

## ğŸ“Š TÃ“M Táº®T LUá»’NG Dá»® LIá»†U

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DATA FLOW SUMMARY                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. SOURCE: CSV (1.3M rows, 23 cols, 0.58% fraud)
              â†“
2. PRODUCER: Read â†’ JSON â†’ Kafka (12 tx/sec)
              â†“
3. KAFKA: Queue (transactions_hsbc topic)
              â†“
4. SPARK STREAMING:
   - Parse JSON (23 cols)
   - Feature Engineering (+21 features â†’ 44 cols total)
   - Split into 2 streams:
     
     Branch A: ARCHIVE          Branch B: INFERENCE
     â†’ MinIO (all data)         â†’ XGBoost Model
       Parquet format             â†’ Predict (21 features)
       Partitioned by date        â†’ Filter (prediction=1)
       For retraining             â†’ Cassandra (fraud only)
              â†“                            â†“
5. STORAGE:                      6. ALERTS:
   MinIO: 100% data                Cassandra: ~0.6-10% fraud
   Permanent                        Permanent
              â†“                            â†“
7. CONSUMPTION:                  8. VISUALIZATION:
   Batch retraining                FastAPI â†’ Streamlit
   Analytics                       Real-time dashboard
   Audit trail                     Alerts monitoring

KEY METRICS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Throughput: 12 transactions/second
â€¢ Latency: <2 seconds (ingestion â†’ detection)
â€¢ Fraud rate: 0.58% (training), ~10% (production demo)
â€¢ Features: 21 engineered features
â€¢ Model: XGBoost (100 trees, AUC=0.9964)
â€¢ Storage: 100% MinIO, ~10% Cassandra
â€¢ Retention: Kafka 7d, MinIO/Cassandra permanent
```

---

## ğŸ“ LÆ¯U Ã QUAN TRá»ŒNG

### âœ… CÃ¡c Äiá»ƒm Cáº§n Nhá»›

1. **Cassandra chá»‰ lÆ°u FRAUD detected**, khÃ´ng pháº£i táº¥t cáº£ transactions
2. **Feature engineering** táº¡o 21 features tá»« 23 columns gá»‘c
3. **Model sá»­ dá»¥ng 21 features**, khÃ´ng pháº£i táº¥t cáº£ 44 columns
4. **Kappa architecture**: Má»™t luá»“ng streaming duy nháº¥t, phÃ¢n nhÃ¡nh sau khi feature engineering
5. **Rate control** á»Ÿ Producer: 12 tx/sec (cÃ³ thá»ƒ config)
6. **Kafka offset**: earliest (Ä‘á»c tá»« Ä‘áº§u) hoáº·c latest (chá»‰ Ä‘á»c má»›i)

### ğŸ¯ Performance Targets

- **Latency**: <2 giÃ¢y tá»« Producer â†’ Cassandra
- **Throughput**: 12 tx/sec sustained
- **Model AUC**: >0.99
- **Fraud detection rate**: ~99% recall
- **False positive rate**: <1%

---

**TÃ i liá»‡u nÃ y mÃ´ táº£ chi tiáº¿t 100% luá»“ng dá»¯ liá»‡u trong há»‡ thá»‘ng HSBC Fraud Detection cá»§a báº¡n.**
