# üìã HSBC FRAUD DETECTION - CHECKLIST V·∫¨N H√ÄNH

## ‚úÖ KH·ªûI ƒê·ªòNG H·ªÜ TH·ªêNG (L·∫ßn ƒë·∫ßu ho·∫∑c sau khi t·∫Øt)

### B∆∞·ªõc 1: Kh·ªüi ƒë·ªông Infrastructure
```powershell
cd A:\hsbc-fraud-detection-new

# Start t·∫•t c·∫£ services
docker compose up -d
```

**ƒê·ª£i 30 gi√¢y** ƒë·ªÉ c√°c services kh·ªüi ƒë·ªông ho√†n t·∫•t.

---

### B∆∞·ªõc 2: Ki·ªÉm tra Services
```powershell
# Xem t·∫•t c·∫£ containers ƒëang ch·∫°y
docker ps

# K·∫øt qu·∫£ mong ƒë·ª£i: 9 containers
# - zookeeper
# - kafka
# - minio
# - cassandra
# - spark-master
# - spark-worker
# - producer
# - api
# - dashboard
```

---

### B∆∞·ªõc 3: Ki·ªÉm tra Cassandra & Data
```powershell
# Xem s·ªë l∆∞·ª£ng fraud alerts
docker exec cassandra cqlsh -e "SELECT COUNT(*) FROM hsbc.fraud_alerts;"

# Xem 5 alerts m·ªõi nh·∫•t
docker exec cassandra cqlsh -e "SELECT * FROM hsbc.fraud_alerts LIMIT 5;"
```

---

### B∆∞·ªõc 4: Ki·ªÉm tra Model v√† Dependencies
```powershell
# Check XGBoost model
docker exec spark-master ls -lh /opt/data/models/fraud_xgb_21features/

# Check XGBoost installed
docker exec spark-master python3 -c "import xgboost; print('XGBoost version:', xgboost.__version__)"

# N·∫øu kh√¥ng c√≥ XGBoost, install:
docker exec spark-master bash -c "pip3 install xgboost scikit-learn pyarrow"

# N·∫øu KH√îNG c√≥ model ‚Üí Ch·∫°y B∆∞·ªõc 5 (Training)
# N·∫øu C√ì model ‚Üí B·ªè qua B∆∞·ªõc 5, chuy·ªÉn sang B∆∞·ªõc 6
```

---

### B∆∞·ªõc 5: Training XGBoost Model (N·∫øu ch∆∞a c√≥ model)
```powershell
# Copy files m·ªõi nh·∫•t v√†o spark-master
docker cp streaming-pipeline/feature_engineering.py spark-master:/opt/spark-apps/
docker cp streaming-pipeline/model_retraining_xgb.py spark-master:/opt/spark-apps/
docker cp streaming-pipeline/unified_streaming.py spark-master:/opt/spark-apps/

# Train XGBoost model (m·∫•t ~6-10 ph√∫t, 1.3M rows)
docker exec spark-master bash -c "cd /opt/spark-apps && export PYSPARK_PYTHON=/usr/bin/python3 && /opt/spark/bin/spark-submit --master local[4] --driver-memory 4g --conf spark.sql.shuffle.partitions=20 /opt/spark-apps/model_retraining_xgb.py"

# Verify model ƒë√£ ƒë∆∞·ª£c t·∫°o
docker exec spark-master ls -lh /opt/data/models/fraud_xgb_21features/
```

**K·∫øt qu·∫£ mong ƒë·ª£i:**
```
‚úÖ Model trained successfully
‚úÖ AUC-ROC: 0.9964
‚úÖ XGBoost model saved to /opt/data/models/fraud_xgb_21features
```

---

### B∆∞·ªõc 6: Start Streaming Pipeline
```powershell
# Kill streaming c≈© n·∫øu c√≥
docker exec spark-master pkill -f unified_streaming

# Start streaming m·ªõi
docker exec -d spark-master bash -c "cd /opt/spark-apps && export PYSPARK_PYTHON=/usr/bin/python3 && /opt/spark/bin/spark-submit --master local[4] --driver-memory 2g --conf spark.sql.shuffle.partitions=20 --conf spark.streaming.kafka.consumer.poll.ms=256 --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,org.apache.hadoop:hadoop-aws:3.3.4,com.datastax.spark:spark-cassandra-connector_2.12:3.4.0 /opt/spark-apps/unified_streaming.py"

# ƒê·ª£i 10 gi√¢y
Start-Sleep -Seconds 10

# Verify streaming ƒëang ch·∫°y
docker exec spark-master ps aux | Select-String "unified_streaming"
```

---

### B∆∞·ªõc 7: Ki·ªÉm tra Producer
```powershell
# Check producer logs
docker logs producer --tail 20

# K·∫øt qu·∫£ mong ƒë·ª£i:
# Rate: 15 tx/sec
# üìä Sent: XXX | Fraud: XX (XX.XX%) | Rate: XX.X tx/sec
```

**N·∫øu producer ch∆∞a ch·∫°y ho·∫∑c b·ªã l·ªói:**
```powershell
docker compose restart producer
```

---

### B∆∞·ªõc 8: Verify End-to-End
```powershell
# 1. Check Cassandra c√≥ nh·∫≠n fraud alerts
docker exec cassandra cqlsh -e "SELECT COUNT(*) FROM hsbc.fraud_alerts;"

# 2. Test API
curl http://localhost:8000/
curl http://localhost:8000/fraud/stats

# 3. Check API logs
docker logs api --tail 10

# 4. Check Dashboard logs
docker logs dashboard --tail 10
```

---

### B∆∞·ªõc 9: M·ªü Dashboard
```
üåê API:        http://localhost:8000
üìä Dashboard:  http://localhost:8501
üéØ Spark UI:   http://localhost:8080
```

**Trong Dashboard:**
- Ch·ªçn Limit = **5000** ho·∫∑c **10000** ƒë·ªÉ xem nhi·ªÅu alerts
- B·∫≠t **Auto-refresh (5s)** ƒë·ªÉ xem real-time

---

## üîÑ RETRAIN MODEL (Khi c·∫ßn update model)

### Chu·∫©n b·ªã
```powershell
# Stop streaming hi·ªán t·∫°i
docker exec spark-master pkill -f unified_streaming

# X√≥a model c≈©
docker exec spark-master rm -rf /opt/data/models/fraud_xgb_21features
```

### Train l·∫°i
```powershell
# Copy files m·ªõi (n·∫øu c√≥ thay ƒë·ªïi)
docker cp streaming-pipeline/feature_engineering.py spark-master:/opt/spark-apps/
docker cp streaming-pipeline/model_retraining_xgb.py spark-master:/opt/spark-apps/

# Install/update XGBoost if needed
docker exec spark-master bash -c "pip3 install xgboost scikit-learn pyarrow"

# Train XGBoost
docker exec spark-master bash -c "cd /opt/spark-apps && export PYSPARK_PYTHON=/usr/bin/python3 && /opt/spark/bin/spark-submit --master local[4] --driver-memory 4g --conf spark.sql.shuffle.partitions=20 /opt/spark-apps/model_retraining_xgb.py"

# ƒê·ª£i ~6-10 ph√∫t ƒë·ªÉ training ho√†n t·∫•t
```

### Restart Streaming
```powershell
# Copy streaming file m·ªõi
docker cp streaming-pipeline/unified_streaming.py spark-master:/opt/spark-apps/

# Start streaming v·ªõi model m·ªõi
docker exec -d spark-master bash -c "cd /opt/spark-apps && export PYSPARK_PYTHON=/usr/bin/python3 && /opt/spark/bin/spark-submit --master local[4] --driver-memory 2g --conf spark.sql.shuffle.partitions=20 --conf spark.streaming.kafka.consumer.poll.ms=256 --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,org.apache.hadoop:hadoop-aws:3.3.4,com.datastax.spark:spark-cassandra-connector_2.12:3.4.0 /opt/spark-apps/unified_streaming.py"
```

---

## üìä MONITORING

### Xem Logs Real-time
```powershell
# Streaming logs
docker logs -f spark-master --tail 50

# Producer logs
docker logs -f producer --tail 30

# API logs
docker logs -f api --tail 20

# Dashboard logs
docker logs -f dashboard --tail 20
```

### Ki·ªÉm tra Performance
```powershell
# Producer throughput
docker logs producer --tail 5

# Fraud detection count
docker exec cassandra cqlsh -e "SELECT COUNT(*) FROM hsbc.fraud_alerts;"

# System resources
docker stats --no-stream
```

---

## üõë STOP H·ªÜ TH·ªêNG

### Stop to√†n b·ªô
```powershell
# Stop t·∫•t c·∫£ containers
docker compose down

# Ho·∫∑c stop nh∆∞ng gi·ªØ data
docker compose stop
```

### Stop t·ª´ng service
```powershell
# Stop streaming
docker exec spark-master pkill -f unified_streaming

# Stop producer
docker compose stop producer

# Stop API/Dashboard
docker compose stop api dashboard
```

---

## üîß TROUBLESHOOTING

### Producer kh√¥ng g·ª≠i data
```powershell
# Restart producer
docker compose restart producer

# Check Kafka
docker logs kafka --tail 20
```

### Streaming kh√¥ng detect fraud
```powershell
# Check XGBoost model path
docker exec spark-master ls -lh /opt/data/models/fraud_xgb_21features/

# Check XGBoost installed
docker exec spark-master python3 -c "import xgboost"

# Install if missing
docker exec spark-master bash -c "pip3 install xgboost scikit-learn pyarrow"

# Restart streaming
docker exec spark-master pkill -f unified_streaming
# R·ªìi start l·∫°i (B∆∞·ªõc 6)
```

### API kh√¥ng tr·∫£ v·ªÅ data
```powershell
# Check Cassandra
docker exec cassandra cqlsh -e "SELECT * FROM hsbc.fraud_alerts LIMIT 5;"

# Restart API
docker compose restart api

# Check API logs
docker logs api --tail 20
```

### Dashboard ch·ªâ hi·ªán 1000 alerts
```powershell
# Copy file ƒë√£ fix
docker cp api/main.py api:/app/main.py
docker cp dashboard/app.py dashboard:/app/app.py

# Restart
docker compose restart api dashboard

# Trong dashboard, ch·ªçn Limit = 5000 ho·∫∑c 10000
```

### Out of Memory
```powershell
# TƒÉng memory cho Spark (edit docker-compose.yml)
# spark-worker:
#   deploy:
#     resources:
#       limits:
#         memory: 4G

docker compose up -d --force-recreate spark-worker
```

---

## üìà TH√îNG S·ªê H·ªÜ TH·ªêNG

### Model Specifications
- **Type**: XGBoost Classifier (Gradient Boosting)
- **Features**: 21 engineered features (numeric, demographic, temporal, geographic, category one-hot)
- **Training Data**: 1,296,675 rows (100% of fraudTrain.csv)
- **Performance**: AUC-ROC 0.9964, Recall ~99%, Precision ~54.6%
- **Hyperparameters**: 100 trees, max_depth=6, learning_rate=0.3, subsample=0.8
- **Model Path**: `/opt/data/models/fraud_xgb_21features`

### Performance Settings
- **Producer Rate**: 12 tx/s (configurable via TRANSACTION_RATE env)
- **Spark Shuffle Partitions**: 20
- **Driver Memory**: 2GB (streaming), 4GB (training)
- **Kafka Consumer Poll**: 256ms
- **Streaming Trigger**: 2 seconds
- **API Limit**: 10,000 records max
- **Dashboard Limit**: 100, 500, 1000, 5000, 10000 options

### Data Paths
- **Training Data**: `./data/raw/fraudTrain.csv` (1.3M rows)
- **Production Data**: `./data/raw/df_test_hdfs.csv` or `df_sampled.csv`
- **Container Data**: `/data/raw/`
- **Model**: `/opt/data/models/fraud_xgb_21features`
- **Cassandra Keyspace**: `hsbc.fraud_alerts`

---

## ‚úÖ QUICK STATUS CHECK

Ch·∫°y l·ªánh n√†y ƒë·ªÉ ki·ªÉm tra nhanh:
```powershell
Write-Host "`n=== HSBC FRAUD DETECTION STATUS ===" -ForegroundColor Cyan
Write-Host "`nüì¶ Containers:" -ForegroundColor Yellow
docker ps --format "table {{.Names}}\t{{.Status}}\t{{.Ports}}" | Select-String "api|dashboard|producer|spark|cassandra|kafka"

Write-Host "`nüö® Fraud Alerts:" -ForegroundColor Yellow
docker exec cassandra cqlsh -e "SELECT COUNT(*) FROM hsbc.fraud_alerts;"

Write-Host "`nüìä Producer:" -ForegroundColor Yellow
docker logs producer --tail 3

Write-Host "`nüéØ URLs:" -ForegroundColor Green
Write-Host "API:       http://localhost:8000"
Write-Host "Dashboard: http://localhost:8501"
Write-Host "Spark UI:  http://localhost:8080"
```

---

## üìù GHI CH√ö

1. **L·∫ßn ƒë·∫ßu ch·∫°y**: Th·ª±c hi·ªán ƒë·∫ßy ƒë·ªß t·ª´ B∆∞·ªõc 1 ‚Üí 9
2. **Ch·∫°y l·∫°i sau khi t·∫Øt**: B∆∞·ªõc 1 ‚Üí B∆∞·ªõc 4 (check model) ‚Üí B∆∞·ªõc 6 ‚Üí 9
3. **Retrain model**: Follow section "RETRAIN MODEL"
4. **N·∫øu c√≥ l·ªói**: Xem section "TROUBLESHOOTING"

**Th·ªùi gian kh·ªüi ƒë·ªông:**
- Infrastructure: ~30 gi√¢y
- Training XGBoost model: ~6-10 ph√∫t (1.3M rows)
- Streaming startup: ~10 gi√¢y
- **T·ªïng**: ~7-11 ph√∫t (l·∫ßn ƒë·∫ßu training model)

**L∆∞u √Ω:**
- Model ƒë√£ train s·∫Ω ƒë∆∞·ª£c l∆∞u persistent trong `./data/models/`
- Cassandra data ƒë∆∞·ª£c l∆∞u trong Docker volume
- Kh√¥ng c·∫ßn retrain n·∫øu model ƒë√£ t·ªìn t·∫°i
