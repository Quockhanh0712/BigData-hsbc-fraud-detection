# üöÄ H∆Ø·ªöNG D·∫™N V·∫¨N H√ÄNH H·ªÜ TH·ªêNG HSBC FRAUD DETECTION

## üìã M·ª•c L·ª•c
1. [T·ªïng Quan H·ªá Th·ªëng](#t·ªïng-quan-h·ªá-th·ªëng)
2. [Y√™u C·∫ßu H·ªá Th·ªëng](#y√™u-c·∫ßu-h·ªá-th·ªëng)
3. [Ki·∫øn Tr√∫c H·ªá Th·ªëng](#ki·∫øn-tr√∫c-h·ªá-th·ªëng)
4. [Kh·ªüi ƒê·ªông H·ªá Th·ªëng](#kh·ªüi-ƒë·ªông-h·ªá-th·ªëng)
5. [V·∫≠n H√†nh T·ª´ng Th√†nh Ph·∫ßn](#v·∫≠n-h√†nh-t·ª´ng-th√†nh-ph·∫ßn)
6. [Gi√°m S√°t & Ki·ªÉm Tra](#gi√°m-s√°t--ki·ªÉm-tra)
7. [T·∫Øt H·ªá Th·ªëng](#t·∫Øt-h·ªá-th·ªëng)
8. [X·ª≠ L√Ω S·ª± C·ªë](#x·ª≠-l√Ω-s·ª±-c·ªë)

---

## 1Ô∏è‚É£ T·ªïng Quan H·ªá Th·ªëng

### M√¥ t·∫£
H·ªá th·ªëng ph√°t hi·ªán giao d·ªãch gian l·∫≠n real-time cho HSBC s·ª≠ d·ª•ng:
- **Apache Kafka**: Message streaming
- **Apache Spark**: Stream processing & ML
- **Cassandra**: NoSQL database
- **MinIO**: Object storage (S3-compatible)
- **FastAPI**: REST API backend
- **Streamlit**: Real-time dashboard

### Lu·ªìng D·ªØ Li·ªáu
```
CSV Data ‚Üí Producer ‚Üí Kafka ‚Üí Spark Streaming ‚Üí ML Model ‚Üí Cassandra ‚Üí API ‚Üí Dashboard
                                      ‚Üì
                                   MinIO (Archive)
```

---

## 2Ô∏è‚É£ Y√™u C·∫ßu H·ªá Th·ªëng

### Ph·∫ßn M·ªÅm
- **Docker Desktop**: v20.10+
- **Docker Compose**: v2.0+
- **RAM**: T·ªëi thi·ªÉu 8GB (khuy·∫øn ngh·ªã 16GB)
- **Disk**: T·ªëi thi·ªÉu 20GB tr·ªëng

### Ki·ªÉm Tra Y√™u C·∫ßu
```powershell
# Ki·ªÉm tra Docker
docker --version

# Ki·ªÉm tra Docker Compose
docker compose version

# Ki·ªÉm tra Docker ƒëang ch·∫°y
docker ps
```

---

## 3Ô∏è‚É£ Ki·∫øn Tr√∫c H·ªá Th·ªëng

### Danh S√°ch Services

| Service | Container | Port | M·ª•c ƒë√≠ch |
|---------|-----------|------|----------|
| Zookeeper | zookeeper | 2181 | Kafka coordination |
| Kafka | kafka | 9092, 29092 | Message broker |
| MinIO | minio | 9000, 9001 | Object storage |
| Cassandra | cassandra | 9042 | Fraud alerts storage |
| Spark Master | spark-master | 8080, 7077, 4040 | Spark cluster manager |
| Spark Worker | spark-worker | 8081 | Spark executor |
| Producer | producer | - | Generate transactions |
| API | api | 8000 | REST API backend |
| Dashboard | dashboard | 8501 | Web UI |

### Network
- **Bridge Network**: `hsbc-network` - K·∫øt n·ªëi t·∫•t c·∫£ containers

### Volumes
- `zookeeper-data`: Zookeeper persistent data
- `kafka-data`: Kafka logs & topics
- `minio-data`: S3 objects (models, archives)
- `cassandra-data`: Cassandra database

---

## 4Ô∏è‚É£ Kh·ªüi ƒê·ªông H·ªá Th·ªëng

### üü¢ OPTION 1: Kh·ªüi ƒê·ªông To√†n B·ªô (Recommended)

```powershell
# Di chuy·ªÉn v√†o th∆∞ m·ª•c project
cd A:\hsbc-fraud-detection-new

# Kh·ªüi ƒë·ªông t·∫•t c·∫£ services
docker compose up -d

# Ki·ªÉm tra tr·∫°ng th√°i
docker compose ps
```

**Th·ªùi gian kh·ªüi ƒë·ªông**: ~2-3 ph√∫t

### üü° OPTION 2: Kh·ªüi ƒê·ªông T·ª´ng Giai ƒêo·∫°n

#### B∆∞·ªõc 1: Infrastructure Layer (Zookeeper, Kafka, MinIO, Cassandra)
```powershell
docker compose up -d zookeeper kafka minio cassandra

# ƒê·ª£i services kh·ªüi ƒë·ªông ho√†n t·∫•t (30-60 gi√¢y)
Start-Sleep -Seconds 60

# Ki·ªÉm tra health
docker ps --filter "name=zookeeper|kafka|minio|cassandra"
```

#### B∆∞·ªõc 2: Processing Layer (Spark Cluster)
```powershell
docker compose up -d spark-master spark-worker

# ƒê·ª£i Spark cluster kh·ªüi ƒë·ªông (20-30 gi√¢y)
Start-Sleep -Seconds 30

# Ki·ªÉm tra Spark UI: http://localhost:8080
```

#### B∆∞·ªõc 3: Data Source (Producer)
```powershell
docker compose up -d producer

# Ki·ªÉm tra producer logs
docker logs -f producer
```

#### B∆∞·ªõc 4: Application Layer (API, Dashboard)
```powershell
docker compose up -d api dashboard

# Ki·ªÉm tra API health
curl http://localhost:8000/

# M·ªü Dashboard: http://localhost:8501
```

### ‚úÖ X√°c Nh·∫≠n Kh·ªüi ƒê·ªông Th√†nh C√¥ng

```powershell
# Xem t·∫•t c·∫£ containers ƒëang ch·∫°y
docker compose ps

# Expected output: 9 containers with status "Up"
```

---

## 5Ô∏è‚É£ V·∫≠n H√†nh T·ª´ng Th√†nh Ph·∫ßn

### üì¶ 5.1 MinIO (Object Storage)

#### Kh·ªüi ƒê·ªông
```powershell
docker compose up -d minio
```

#### Truy C·∫≠p
- **Console**: http://localhost:9001
- **Username**: `admin`
- **Password**: `password123`

#### Ki·ªÉm Tra Buckets
```powershell
# List buckets
docker exec minio mc ls myminio/

# Xem n·ªôi dung bucket
docker exec minio mc ls -r myminio/hsbc-data/

# Xem models ƒë√£ l∆∞u
docker exec minio mc ls myminio/hsbc-data/models/
```

#### T·∫°o Bucket (n·∫øu ch∆∞a c√≥)
```powershell
docker exec minio mc mb myminio/hsbc-data
```

#### Logs
```powershell
docker logs -f minio
```

#### T·∫Øt
```powershell
docker compose stop minio
```

---

### üóÑÔ∏è 5.2 Cassandra (Database)

#### Kh·ªüi ƒê·ªông
```powershell
docker compose up -d cassandra

# ƒê·ª£i Cassandra ho√†n to√†n ready (~60 gi√¢y)
Start-Sleep -Seconds 60
```

#### Ki·ªÉm Tra Health
```powershell
docker exec -it cassandra cqlsh -e "SELECT now() FROM system.local;"
```

#### T·∫°o Keyspace & Table
```powershell
# T·∫°o keyspace
docker exec -it cassandra cqlsh -e "CREATE KEYSPACE IF NOT EXISTS hsbc WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1};"

# T·∫°o b·∫£ng fraud_alerts
docker exec -it cassandra cqlsh -e "CREATE TABLE IF NOT EXISTS hsbc.fraud_alerts (
    transaction_id text PRIMARY KEY,
    transaction_time timestamp,
    amount double,
    merchant text,
    category text,
    cc_num text,
    first text,
    last text,
    gender text,
    job text,
    state text,
    city text,
    zip text,
    is_fraud double,
    detected_at timestamp
);"
```

#### Truy V·∫•n D·ªØ Li·ªáu
```powershell
# ƒê·∫øm t·ªïng s·ªë fraud alerts
docker exec -it cassandra cqlsh -e "SELECT COUNT(*) FROM hsbc.fraud_alerts;"

# Xem 10 fraud alerts g·∫ßn nh·∫•t
docker exec -it cassandra cqlsh -e "SELECT transaction_id, amount, category, merchant, detected_at FROM hsbc.fraud_alerts LIMIT 10;"

# Query theo category
docker exec -it cassandra cqlsh -e "SELECT * FROM hsbc.fraud_alerts WHERE category='grocery_pos' LIMIT 5 ALLOW FILTERING;"
```

#### X√≥a D·ªØ Li·ªáu (n·∫øu c·∫ßn reset)
```powershell
# Truncate table
docker exec -it cassandra cqlsh -e "TRUNCATE hsbc.fraud_alerts;"

# Drop table
docker exec -it cassandra cqlsh -e "DROP TABLE IF EXISTS hsbc.fraud_alerts;"
```

#### CQL Shell Interactive
```powershell
# V√†o CQL shell
docker exec -it cassandra cqlsh

# Trong CQL shell:
USE hsbc;
DESCRIBE TABLES;
SELECT * FROM fraud_alerts LIMIT 5;
EXIT;
```

#### Logs
```powershell
docker logs -f cassandra
```

#### T·∫Øt
```powershell
docker compose stop cassandra
```

---

### üì® 5.3 Kafka (Message Broker)

#### Kh·ªüi ƒê·ªông
```powershell
# Kh·ªüi ƒë·ªông Zookeeper tr∆∞·ªõc
docker compose up -d zookeeper
Start-Sleep -Seconds 10

# Kh·ªüi ƒë·ªông Kafka
docker compose up -d kafka
Start-Sleep -Seconds 20
```

#### Ki·ªÉm Tra Topics
```powershell
# List t·∫•t c·∫£ topics
docker exec kafka kafka-topics --list --bootstrap-server localhost:9092

# Xem chi ti·∫øt topic
docker exec kafka kafka-topics --describe --topic transactions_hsbc --bootstrap-server localhost:9092
```

#### T·∫°o Topic Th·ªß C√¥ng (n·∫øu c·∫ßn)
```powershell
docker exec kafka kafka-topics --create \
  --topic transactions_hsbc \
  --bootstrap-server localhost:9092 \
  --partitions 3 \
  --replication-factor 1
```

#### Consume Messages (ƒë·ªÉ test)
```powershell
# Xem messages t·ª´ ƒë·∫ßu topic
docker exec kafka kafka-console-consumer \
  --topic transactions_hsbc \
  --from-beginning \
  --bootstrap-server localhost:9092 \
  --max-messages 10
```

#### Produce Test Message
```powershell
# G·ª≠i test message
docker exec -it kafka kafka-console-producer \
  --topic transactions_hsbc \
  --bootstrap-server localhost:9092

# Nh·∫≠p JSON v√† Enter, Ctrl+C ƒë·ªÉ tho√°t
```

#### Monitor Consumer Groups
```powershell
# List consumer groups
docker exec kafka kafka-consumer-groups --list --bootstrap-server localhost:9092

# Xem chi ti·∫øt group
docker exec kafka kafka-consumer-groups --describe \
  --group spark-kafka-streaming \
  --bootstrap-server localhost:9092
```

#### Logs
```powershell
docker logs -f kafka
```

#### T·∫Øt
```powershell
docker compose stop kafka zookeeper
```

---

### ‚ö° 5.4 Spark Cluster (Processing)

#### Kh·ªüi ƒê·ªông Cluster
```powershell
# Kh·ªüi ƒë·ªông Master
docker compose up -d spark-master
Start-Sleep -Seconds 15

# Kh·ªüi ƒë·ªông Worker
docker compose up -d spark-worker
Start-Sleep -Seconds 10
```

#### Ki·ªÉm Tra Cluster
```powershell
# Spark Master UI: http://localhost:8080
# Workers tab should show 1 worker v·ªõi 12 cores, 1024.0 MB RAM

# Ki·ªÉm tra t·ª´ command line
docker exec spark-master curl -s http://localhost:8080 | Select-String "Workers"
```

#### Restart Cluster (khi c·∫ßn)
```powershell
# Stop all Spark processes
docker exec spark-master /opt/spark/sbin/stop-all.sh

# Restart containers
docker compose restart spark-master spark-worker

# ƒê·ª£i cluster ready
Start-Sleep -Seconds 20
```

#### Copy Code v√†o Spark Master
```powershell
# Copy streaming pipeline
docker cp streaming-pipeline/unified_streaming.py spark-master:/opt/spark-apps/

# Copy feature engineering
docker cp streaming-pipeline/feature_engineering.py spark-master:/opt/spark-apps/

# Copy XGBoost model retraining
docker cp streaming-pipeline/model_retraining_xgb.py spark-master:/opt/spark-apps/
```

#### Submit Streaming Job
```powershell
docker exec spark-master /opt/spark/bin/spark-submit \
  --master spark://spark-master:7077 \
  --deploy-mode client \
  --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,com.datastax.spark:spark-cassandra-connector_2.12:3.4.0,org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262 \
  /opt/spark-apps/unified_streaming.py
```

**Output mong ƒë·ª£i**:
```
INFO:__main__:‚úÖ Spark session created: 3.5.0
INFO:__main__:Loading XGBoost model from /opt/data/models/fraud_xgb_21features
INFO:__main__:‚úÖ Model loaded successfully
INFO:__main__:‚úÖ Subscribed to topic: transactions_hsbc
INFO:__main__:‚úÖ Archive stream started ‚Üí s3a://hsbc-data/stream-archive/
INFO:__main__:‚úÖ Inference stream started
INFO:__main__:‚úÖ ALL STREAMS STARTED SUCCESSFULLY
INFO:__main__:üì¶ Batch 1: 100 transactions
INFO:__main__:üö® Batch 1: Detected 2 fraud alerts ‚Üí Cassandra
INFO:__main__:üö® FRAUD DETECTED: Transaction abc123, Amount: $285.54
```

#### D·ª´ng Streaming Job
```powershell
# Kill process
docker exec spark-master pkill -f unified_streaming.py

# Ho·∫∑c Ctrl+C n·∫øu ch·∫°y foreground
```

#### Spark UI
- **Master UI**: http://localhost:8080 - Cluster status, workers
- **Application UI**: http://localhost:4040 - Job progress, stages, executors

#### Logs
```powershell
# Spark Master logs
docker logs -f spark-master

# Spark Worker logs
docker logs -f spark-worker

# Application logs (khi job ƒëang ch·∫°y)
docker exec spark-master cat /opt/spark/work/*/stdout
```

#### T·∫Øt
```powershell
docker compose stop spark-master spark-worker
```

---

### üîÑ 5.5 Producer (Transaction Generator)

#### Kh·ªüi ƒê·ªông
```powershell
docker compose up -d producer
```

#### C·∫•u H√¨nh
File: `producer/config.py`
```python
KAFKA_BOOTSTRAP_SERVERS = 'kafka:29092'
KAFKA_TRANSACTION_TOPIC = 'transactions_hsbc'
TRANSACTION_RATE = 2  # transactions/second
CSV_FILE = '/data/raw/fraudTrain.csv'
```

#### Thay ƒê·ªïi Transaction Rate
```powershell
# Edit docker-compose.yml
# environment:
#   TRANSACTION_RATE: 5  # tƒÉng l√™n 5 tx/s

docker compose restart producer
```

#### Ki·ªÉm Tra Ho·∫°t ƒê·ªông
```powershell
# Xem logs real-time
docker logs -f producer

# Expected output:
# üì§ Sent transaction: {'trans_num': 'abc123', 'amount': 45.67, ...}
# üì§ Sent 100 transactions (rate: 2.0/s)
```

#### Stop/Start
```powershell
# T·∫°m d·ª´ng g·ª≠i transactions
docker compose stop producer

# Ti·∫øp t·ª•c
docker compose start producer
```

#### Logs
```powershell
docker logs -f producer --tail 50
```

#### T·∫Øt
```powershell
docker compose stop producer
```

---

### üîß 5.6 Model Training (One-time/Periodic)

#### Chu·∫©n B·ªã
```powershell
# Copy training script v√†o Spark Master
docker cp streaming-pipeline/model_retraining.py spark-master:/opt/spark-apps/
docker cp streaming-pipeline/feature_engineering.py spark-master:/opt/spark-apps/
```

#### Ch·∫°y XGBoost Training
```powershell
docker exec spark-master bash -c "cd /opt/spark-apps && export PYSPARK_PYTHON=/usr/bin/python3 && /opt/spark/bin/spark-submit --master local[4] --driver-memory 4g --conf spark.sql.shuffle.partitions=20 /opt/spark-apps/model_retraining_xgb.py"
```

#### Training Process
```
1. Load data: fraudTrain.csv (100% - 1,296,675 rows)
2. Feature engineering: 21 features (numeric, demographic, temporal, geographic, category)
3. Train XGBoost: 100 trees, depth 6, learning_rate 0.3
4. Evaluate: AUC-ROC 0.9964, Recall ~99%, Precision ~54.6%
5. Save model: /opt/data/models/fraud_xgb_21features
```

**Th·ªùi gian**: ~6-10 ph√∫t (1.3M rows)

#### Ki·ªÉm Tra Model ƒê√£ L∆∞u
```powershell
# Ki·ªÉm tra XGBoost model
docker exec spark-master ls -lh /opt/data/models/fraud_xgb_21features

# Xem metadata
docker exec spark-master cat /opt/data/models/fraud_xgb_21features/metadata/part-00000

# Check XGBoost version
docker exec spark-master python3 -c "import xgboost; print('XGBoost:', xgboost.__version__)"
```

#### Model Location
- **Container path**: `/opt/data/models/fraud_xgb_21features`
- **Host path**: `./data/models/fraud_xgb_21features`

---

### üåê 5.7 API Backend (FastAPI)

#### Kh·ªüi ƒê·ªông
```powershell
docker compose up -d api
```

#### Ki·ªÉm Tra Health
```powershell
# Health check
curl http://localhost:8000/

# Expected: {"service":"HSBC Fraud Detection API","status":"running","version":"1.0.0"}
```

#### API Endpoints

##### 1. Health Check
```powershell
curl http://localhost:8000/
```

##### 2. Get Fraud Alerts (with filters)
```powershell
# Get 10 latest alerts
curl http://localhost:8000/fraud/alerts?limit=10

# Filter by category
curl http://localhost:8000/fraud/alerts?category=grocery_pos&limit=20

# Filter by state
curl http://localhost:8000/fraud/alerts?state=CA&limit=15

# PowerShell v·ªõi formatted output
(curl http://localhost:8000/fraud/alerts?limit=5).Content | ConvertFrom-Json | ConvertTo-Json -Depth 3
```

##### 3. Get Statistics
```powershell
curl http://localhost:8000/fraud/stats

# PowerShell formatted
(curl http://localhost:8000/fraud/stats).Content | ConvertFrom-Json | ConvertTo-Json -Depth 3
```

##### 4. Get Total Count
```powershell
curl http://localhost:8000/fraud/count
```

#### API Documentation
- **Swagger UI**: http://localhost:8000/docs
- **ReDoc**: http://localhost:8000/redoc

#### Logs
```powershell
# Real-time logs
docker logs -f api

# Last 50 lines
docker logs api --tail 50
```

#### Restart (sau khi thay ƒë·ªïi code)
```powershell
# Rebuild image
docker compose build api

# Restart container
docker compose restart api
```

#### T·∫Øt
```powershell
docker compose stop api
```

---

### üìä 5.8 Dashboard (Streamlit)

#### Kh·ªüi ƒê·ªông
```powershell
docker compose up -d dashboard
```

#### Truy C·∫≠p
- **URL**: http://localhost:8501
- **Auto-open browser**: Dashboard t·ª± ƒë·ªông m·ªü khi container start

#### Features
1. **Overview Metrics**
   - Total fraud alerts
   - Total amount ($)
   - Average amount ($)

2. **Fraud Alerts Table**
   - Sortable columns
   - Filters: Limit (10/25/50/100), Category
   - Real-time data

3. **Analytics Charts**
   - Bar chart: Fraud by category
   - Bar chart: Top 10 states
   - Histogram: Amount distribution

4. **Auto-Refresh**
   - Checkbox ƒë·ªÉ enable
   - Refresh interval: 5 seconds

#### S·ª≠ D·ª•ng Dashboard

```
1. M·ªü http://localhost:8501
2. Quan s√°t Overview metrics ·ªü top
3. Scroll xu·ªëng xem Fraud Alerts table
4. S·ª≠ d·ª•ng filters:
   - "Number of alerts to display": ch·ªçn 10/25/50/100
   - "Filter by category": ch·ªçn category c·ª• th·ªÉ
5. Xem Analytics charts b√™n d∆∞·ªõi
6. Check "Auto-refresh (5s)" ƒë·ªÉ c·∫≠p nh·∫≠t real-time
```

#### Thay ƒê·ªïi C·∫•u H√¨nh
File: `dashboard/app.py`

```python
# API URL
API_URL = os.getenv('API_URL', 'http://api:8000')

# Refresh interval (seconds)
time.sleep(5)  # thay ƒë·ªïi s·ªë gi√¢y ·ªü ƒë√¢y
```

#### Rebuild Dashboard (sau khi thay ƒë·ªïi)
```powershell
docker compose build dashboard
docker compose restart dashboard
```

#### Logs
```powershell
docker logs -f dashboard
```

#### T·∫Øt
```powershell
docker compose stop dashboard
```

---

## 6Ô∏è‚É£ Gi√°m S√°t & Ki·ªÉm Tra

### üîç 6.1 Container Health

#### Xem T·∫•t C·∫£ Containers
```powershell
docker compose ps
```

#### Ki·ªÉm Tra Resource Usage
```powershell
docker stats

# Specific containers
docker stats kafka spark-master spark-worker cassandra
```

#### Container Status
```powershell
# Check if running
docker ps --filter "name=api"

# Check exit code
docker compose ps api
```

---

### üìà 6.2 Logs Monitoring

#### Real-time Logs (All Services)
```powershell
docker compose logs -f
```

#### Specific Service Logs
```powershell
# Producer (xem transactions ƒëang g·ª≠i)
docker logs -f producer --tail 100

# Spark Master (xem job progress)
docker logs -f spark-master --tail 200

# API (xem API requests)
docker logs -f api --tail 50

# Dashboard (xem Streamlit activity)
docker logs -f dashboard --tail 30
```

#### Search Logs
```powershell
# T√¨m errors
docker logs api 2>&1 | Select-String "ERROR"

# T√¨m fraud detections
docker logs spark-master 2>&1 | Select-String "fraud alerts"

# T√¨m batch processing
docker logs spark-master 2>&1 | Select-String "Batch"
```

---

### üéØ 6.3 Data Flow Verification

#### 1. Kafka Messages
```powershell
# Xem messages trong topic
docker exec kafka kafka-console-consumer \
  --topic transactions_hsbc \
  --from-beginning \
  --bootstrap-server localhost:9092 \
  --max-messages 5
```

#### 2. Spark Processing
```powershell
# Xem Spark UI
# http://localhost:4040 (khi streaming job ƒëang ch·∫°y)

# Check batch processing trong logs
docker logs spark-master --tail 50 | Select-String "Batch"
```

#### 3. Cassandra Data
```powershell
# ƒê·∫øm fraud alerts
docker exec -it cassandra cqlsh -e "SELECT COUNT(*) FROM hsbc.fraud_alerts;"

# Xem latest alerts
docker exec -it cassandra cqlsh -e "SELECT transaction_id, amount, category, detected_at FROM hsbc.fraud_alerts LIMIT 10;"
```

#### 4. MinIO Archives
```powershell
# List archived files
docker exec minio mc ls -r myminio/hsbc-data/stream-archive/

# Xem s·ªë l∆∞·ª£ng files
docker exec minio mc du myminio/hsbc-data/stream-archive/
```

#### 5. API Response
```powershell
# Test API endpoints
curl http://localhost:8000/fraud/count
curl http://localhost:8000/fraud/stats
```

#### 6. Dashboard Visibility
- M·ªü http://localhost:8501
- Ki·ªÉm tra metrics ƒëang update
- Verify charts hi·ªÉn th·ªã data

---

### üö® 6.4 Health Checks

#### Automated Health Check Script
```powershell
# T·∫°o file health_check.ps1
@"
Write-Host "üîç HSBC Fraud Detection - Health Check" -ForegroundColor Cyan

# 1. Containers
Write-Host "`n‚úÖ Container Status:" -ForegroundColor Green
docker compose ps

# 2. Kafka
Write-Host "`n‚úÖ Kafka Topics:" -ForegroundColor Green
docker exec kafka kafka-topics --list --bootstrap-server localhost:9092

# 3. Cassandra
Write-Host "`n‚úÖ Cassandra Fraud Count:" -ForegroundColor Green
docker exec cassandra cqlsh -e "SELECT COUNT(*) FROM hsbc.fraud_alerts;"

# 4. API
Write-Host "`n‚úÖ API Health:" -ForegroundColor Green
curl http://localhost:8000/ | ConvertFrom-Json

# 5. MinIO
Write-Host "`n‚úÖ MinIO Buckets:" -ForegroundColor Green
docker exec minio mc ls myminio/

Write-Host "`n‚úÖ Health Check Complete!" -ForegroundColor Cyan
"@ | Out-File health_check.ps1

# Ch·∫°y health check
.\health_check.ps1
```

---

## 7Ô∏è‚É£ T·∫Øt H·ªá Th·ªëng

### üõë 7.1 Stop All Services (Gi·ªØ Data)

```powershell
# D·ª´ng t·∫•t c·∫£ containers (data v·∫´n ƒë∆∞·ª£c gi·ªØ trong volumes)
docker compose stop

# Ho·∫∑c d·ª´ng t·ª´ng service c·ª• th·ªÉ
docker compose stop producer
docker compose stop spark-master spark-worker
docker compose stop api dashboard
```

### üóëÔ∏è 7.2 Down All Services (X√≥a Containers, Gi·ªØ Volumes)

```powershell
# X√≥a containers nh∆∞ng GI·ªÆ volumes (data)
docker compose down

# Ki·ªÉm tra volumes v·∫´n c√≤n
docker volume ls | Select-String "hsbc"
```

### üí• 7.3 Complete Cleanup (X√≥a T·∫•t C·∫£, K·ªÉ C·∫£ Data)

```powershell
# ‚ö†Ô∏è C·∫¢NH B√ÅO: L·ªánh n√†y s·∫Ω X√ìA TO√ÄN B·ªò D·ªÆ LI·ªÜU

# Down v√† x√≥a volumes
docker compose down -v

# X√≥a images (optional)
docker compose down -v --rmi all

# X√≥a orphan containers
docker compose down -v --remove-orphans
```

### üì¶ 7.4 Backup Data Tr∆∞·ªõc Khi T·∫Øt

#### Backup Cassandra
```powershell
# Export fraud_alerts table
docker exec cassandra cqlsh -e "COPY hsbc.fraud_alerts TO '/tmp/fraud_alerts_backup.csv' WITH HEADER=TRUE;"

# Copy ra host
docker cp cassandra:/tmp/fraud_alerts_backup.csv ./backups/fraud_alerts_$(Get-Date -Format 'yyyyMMdd_HHmmss').csv
```

#### Backup MinIO
```powershell
# Sync bucket to local
docker exec minio mc mirror myminio/hsbc-data ./backups/minio-backup/
```

#### Backup XGBoost Model
```powershell
# Copy XGBoost model directory
docker cp spark-master:/opt/data/models/fraud_xgb_21features ./backups/model_$(Get-Date -Format 'yyyyMMdd_HHmmss')
```

---

## 8Ô∏è‚É£ X·ª≠ L√Ω S·ª± C·ªë

### ‚ùå 8.1 Container Kh√¥ng Kh·ªüi ƒê·ªông

#### Tri·ªáu ch·ª©ng
```powershell
docker compose ps
# Output: Container status = "Exited (1)"
```

#### Gi·∫£i ph√°p
```powershell
# 1. Xem logs ƒë·ªÉ t√¨m l·ªói
docker logs <container_name>

# 2. Ki·ªÉm tra port conflicts
netstat -ano | Select-String "8080|9092|9042|8000|8501"

# 3. Restart container
docker compose restart <service_name>

# 4. Rebuild n·∫øu c·∫ßn
docker compose build <service_name>
docker compose up -d <service_name>
```

---

### ‚ö†Ô∏è 8.2 Spark Job Kh√¥ng Ch·∫°y

#### Tri·ªáu ch·ª©ng
```
Initial job has not accepted any resources
```

#### Gi·∫£i ph√°p
```powershell
# 1. Restart Spark cluster
docker compose restart spark-master spark-worker

# 2. ƒê·ª£i cluster ready
Start-Sleep -Seconds 30

# 3. Ki·ªÉm tra Spark Master UI
# http://localhost:8080 - ph·∫£i th·∫•y 1 worker

# 4. Submit l·∫°i job
docker exec spark-master /opt/spark/bin/spark-submit \
  --master spark://spark-master:7077 \
  --deploy-mode client \
  --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,com.datastax.spark:spark-cassandra-connector_2.12:3.4.0,org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262 \
  /opt/spark-apps/unified_streaming.py
```

---

### üîå 8.3 Kafka Connection Issues

#### Tri·ªáu ch·ª©ng
```
Connection refused: kafka:29092
```

#### Gi·∫£i ph√°p
```powershell
# 1. Ki·ªÉm tra Kafka ƒëang ch·∫°y
docker ps --filter "name=kafka"

# 2. Ki·ªÉm tra Zookeeper
docker ps --filter "name=zookeeper"

# 3. Restart Kafka stack
docker compose restart zookeeper kafka

# 4. ƒê·ª£i Kafka ready (~30s)
Start-Sleep -Seconds 30

# 5. Test connection
docker exec kafka kafka-topics --list --bootstrap-server localhost:9092
```

---

### üóÑÔ∏è 8.4 Cassandra Not Ready

#### Tri·ªáu ch·ª©ng
```
NoHostAvailable: ('Unable to connect to any servers')
```

#### Gi·∫£i ph√°p
```powershell
# 1. Ki·ªÉm tra Cassandra health
docker exec cassandra cqlsh -e "SELECT now() FROM system.local;"

# 2. N·∫øu fail, restart Cassandra
docker compose restart cassandra

# 3. ƒê·ª£i ready (~60s)
Start-Sleep -Seconds 60

# 4. Test l·∫°i
docker exec -it cassandra cqlsh -e "DESCRIBE KEYSPACES;"

# 5. Recreate table n·∫øu c·∫ßn
docker exec -it cassandra cqlsh -e "CREATE TABLE IF NOT EXISTS hsbc.fraud_alerts (
    transaction_id text PRIMARY KEY,
    transaction_time timestamp,
    amount double,
    merchant text,
    category text,
    cc_num text,
    first text,
    last text,
    gender text,
    job text,
    state text,
    city text,
    zip text,
    is_fraud double,
    detected_at timestamp
);"
```

---

### üíæ 8.5 Model Not Found

#### Tri·ªáu ch·ª©ng
```
Model not found at /opt/data/models/fraud_rf_lean
```

#### Gi·∫£i ph√°p
```powershell
# 1. Ki·ªÉm tra model c√≥ t·ªìn t·∫°i kh√¥ng
docker exec spark-master ls -lh /opt/data/models/fraud_rf_lean

# 2. N·∫øu kh√¥ng c√≥, ch·∫°y l·∫°i training
docker exec spark-master /opt/spark/bin/spark-submit \
  --master spark://spark-master:7077 \
  --deploy-mode client \
  /opt/spark-apps/model_retraining.py

# 3. ƒê·ª£i training ho√†n t·∫•t (~5-10 ph√∫t)

# 4. Verify model ƒë√£ ƒë∆∞·ª£c t·∫°o
docker exec spark-master ls -lh /opt/data/models/fraud_rf_lean
```

---

### üåê 8.6 API/Dashboard Connection Issues

#### Tri·ªáu ch·ª©ng
Dashboard kh√¥ng hi·ªÉn th·ªã data ho·∫∑c API tr·∫£ v·ªÅ errors

#### Gi·∫£i ph√°p
```powershell
# 1. Ki·ªÉm tra API ƒëang ch·∫°y
curl http://localhost:8000/

# 2. Test API endpoints
curl http://localhost:8000/fraud/count
curl http://localhost:8000/fraud/alerts?limit=5

# 3. Ki·ªÉm tra Cassandra connection t·ª´ API
docker logs api --tail 50

# 4. Restart API
docker compose restart api

# 5. Restart Dashboard
docker compose restart dashboard

# 6. Clear browser cache v√† refresh http://localhost:8501
```

---

### üîÑ 8.7 Complete System Reset

Khi m·ªçi th·ª© fail, reset to√†n b·ªô h·ªá th·ªëng:

```powershell
# 1. Stop t·∫•t c·∫£
docker compose down

# 2. X√≥a volumes (‚ö†Ô∏è m·∫•t data)
docker compose down -v

# 3. Clean Docker system
docker system prune -a --volumes -f

# 4. Rebuild t·ª´ ƒë·∫ßu
docker compose build --no-cache

# 5. Start l·∫°i
docker compose up -d

# 6. ƒê·ª£i t·∫•t c·∫£ services ready (~3 ph√∫t)
Start-Sleep -Seconds 180

# 7. Setup Cassandra
docker exec -it cassandra cqlsh -e "CREATE KEYSPACE IF NOT EXISTS hsbc WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1};"
docker exec -it cassandra cqlsh -e "CREATE TABLE IF NOT EXISTS hsbc.fraud_alerts (
    transaction_id text PRIMARY KEY,
    transaction_time timestamp,
    amount double,
    merchant text,
    category text,
    cc_num text,
    first text,
    last text,
    gender text,
    job text,
    state text,
    city text,
    zip text,
    is_fraud double,
    detected_at timestamp
);"

# 8. Train model
docker cp streaming-pipeline/model_retraining.py spark-master:/opt/spark-apps/
docker cp streaming-pipeline/feature_engineering.py spark-master:/opt/spark-apps/
docker exec spark-master /opt/spark/bin/spark-submit \
  --master spark://spark-master:7077 \
  --deploy-mode client \
  /opt/spark-apps/model_retraining.py

# 9. Start streaming
docker cp streaming-pipeline/unified_streaming.py spark-master:/opt/spark-apps/
docker exec spark-master /opt/spark/bin/spark-submit \
  --master spark://spark-master:7077 \
  --deploy-mode client \
  --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,com.datastax.spark:spark-cassandra-connector_2.12:3.4.0,org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262 \
  /opt/spark-apps/unified_streaming.py

# 10. Verify dashboard: http://localhost:8501
```

---

## üìù Quick Reference Commands

### Start System
```powershell
docker compose up -d
```

### Stop System
```powershell
docker compose stop
```

### View Logs
```powershell
docker compose logs -f [service_name]
```

### Health Check
```powershell
docker compose ps
curl http://localhost:8000/
```

### Access UIs
- Spark Master: http://localhost:8080
- MinIO Console: http://localhost:9001
- API Docs: http://localhost:8000/docs
- Dashboard: http://localhost:8501

### Clean Restart
```powershell
docker compose down
docker compose up -d
```

---

## üéì Lu·ªìng L√†m Vi·ªác Ti√™u Bi·ªÉu

### Scenario 1: Kh·ªüi ƒê·ªông H·ªá Th·ªëng M·ªõi (L·∫ßn ƒê·∫ßu)

```powershell
# 1. Clone/setup project
cd A:\hsbc-fraud-detection-new

# 2. Start infrastructure
docker compose up -d

# 3. ƒê·ª£i services ready
Start-Sleep -Seconds 120

# 4. Setup Cassandra
docker exec -it cassandra cqlsh -e "CREATE KEYSPACE IF NOT EXISTS hsbc WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1};"
docker exec -it cassandra cqlsh -e "CREATE TABLE IF NOT EXISTS hsbc.fraud_alerts (...);"

# 5. Train model
docker cp streaming-pipeline/model_retraining.py spark-master:/opt/spark-apps/
docker cp streaming-pipeline/feature_engineering.py spark-master:/opt/spark-apps/
docker exec spark-master /opt/spark/bin/spark-submit --master spark://spark-master:7077 /opt/spark-apps/model_retraining.py

# 6. Start streaming
docker cp streaming-pipeline/unified_streaming.py spark-master:/opt/spark-apps/
docker exec spark-master /opt/spark/bin/spark-submit --master spark://spark-master:7077 --packages [...] /opt/spark-apps/unified_streaming.py

# 7. Access dashboard
# http://localhost:8501
```

---

### Scenario 2: Restart H·ªá Th·ªëng ƒê√£ Setup

```powershell
# 1. Start all services
docker compose up -d

# 2. ƒê·ª£i ready
Start-Sleep -Seconds 60

# 3. Start streaming (model ƒë√£ c√≥)
docker cp streaming-pipeline/unified_streaming.py spark-master:/opt/spark-apps/
docker exec spark-master /opt/spark/bin/spark-submit --master spark://spark-master:7077 --packages [...] /opt/spark-apps/unified_streaming.py

# 4. Monitor
docker logs -f spark-master
```

---

### Scenario 3: Update Code & Redeploy

```powershell
# 1. Stop streaming job
docker exec spark-master pkill -f unified_streaming.py

# 2. Update code files (edit locally)

# 3. Copy updated files
docker cp streaming-pipeline/unified_streaming.py spark-master:/opt/spark-apps/
docker cp streaming-pipeline/feature_engineering.py spark-master:/opt/spark-apps/

# 4. Restart streaming
docker exec spark-master /opt/spark/bin/spark-submit --master spark://spark-master:7077 --packages [...] /opt/spark-apps/unified_streaming.py

# 5. Verify logs
docker logs -f spark-master
```

---

### Scenario 4: Retrain Model v·ªõi Data M·ªõi

```powershell
# 1. Upload new training data
# Copy CSV v√†o ./data/raw/fraudTrain.csv

# 2. Stop streaming job
docker exec spark-master pkill -f unified_streaming.py

# 3. Run training
docker exec spark-master /opt/spark/bin/spark-submit --master spark://spark-master:7077 /opt/spark-apps/model_retraining.py

# 4. Restart streaming v·ªõi model m·ªõi
docker exec spark-master /opt/spark/bin/spark-submit --master spark://spark-master:7077 --packages [...] /opt/spark-apps/unified_streaming.py

# 5. Monitor accuracy
docker logs -f spark-master | Select-String "fraud"
```

---

## üìû Support & Resources

### Logs Location
- **Container logs**: `docker logs <container_name>`
- **Spark logs**: `docker exec spark-master ls /opt/spark/work/`
- **Application logs**: Real-time via `docker logs -f`

### Configuration Files
- **Docker Compose**: `docker-compose.yml`
- **Producer**: `producer/config.py`
- **API**: `api/main.py`, `api/database.py`
- **Dashboard**: `dashboard/app.py`
- **Streaming**: `streaming-pipeline/unified_streaming.py`

### Useful Links
- Docker Documentation: https://docs.docker.com/
- Apache Spark: https://spark.apache.org/docs/3.5.0/
- FastAPI: https://fastapi.tiangolo.com/
- Streamlit: https://docs.streamlit.io/

---

**üéâ H·ªá th·ªëng HSBC Fraud Detection ƒë√£ s·∫µn s√†ng!**

M·ªçi th·∫Øc m·∫Øc ho·∫∑c issues, vui l√≤ng ki·ªÉm tra section "X·ª≠ L√Ω S·ª± C·ªë" ho·∫∑c xem logs chi ti·∫øt.
