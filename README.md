# ğŸ¦ HSBC Fraud Detection System - Real-time ML Pipeline

## ğŸ“‹ Tá»•ng quan dá»± Ã¡n

Há»‡ thá»‘ng phÃ¡t hiá»‡n giao dá»‹ch gian láº­n thá»i gian thá»±c sá»­ dá»¥ng Apache Spark, Kafka, vÃ  Machine Learning vá»›i kiáº¿n trÃºc Kappa Architecture.

### ğŸ¯ Má»¥c tiÃªu
- PhÃ¡t hiá»‡n giao dá»‹ch gian láº­n real-time vá»›i Ä‘á»™ chÃ­nh xÃ¡c cao (AUC-ROC: 0.9964)
- Xá»­ lÃ½ streaming data vá»›i Ä‘á»™ trá»… tháº¥p (<1s)
- Scale Ä‘Æ°á»£c vá»›i hÃ ng nghÃ¬n transactions/giÃ¢y
- Monitoring vÃ  alerting tá»± Ä‘á»™ng

### ğŸ—ï¸ Kiáº¿n trÃºc há»‡ thá»‘ng

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Producer  â”‚â”€â”€â”€â”€â”€â–¶â”‚  Kafka  â”‚â”€â”€â”€â”€â”€â–¶â”‚ Spark Stream â”‚â”€â”€â”€â”€â”€â–¶â”‚ Cassandra â”‚
â”‚ (df_test)   â”‚      â”‚         â”‚      â”‚  (XGBoost)   â”‚      â”‚  (Alerts) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                              â”‚
                                              â–¼
                                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                       â”‚   MinIO/S3   â”‚
                                       â”‚  (Archive)   â”‚
                                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                              â”‚
                                              â–¼
                                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                       â”‚  Streamlit   â”‚
                                       â”‚  Dashboard   â”‚
                                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“Š Káº¿t quáº£ Model Performance

| Model | AUC-ROC | Recall | Precision | FPR | Status |
|-------|---------|--------|-----------|-----|--------|
| DecisionTree | 0.8221 | - | - | - | âœ… Trained |
| RandomForest | - | - | - | - | â³ Ready |
| **XGBoost** | **0.9964** | **~100%** | **~93%** | **0.8%** | **ğŸš€ Production** |

### XGBoost Model Configuration
- **Features**: 21 engineered features
- **Estimators**: 100 trees
- **Max Depth**: 6
- **Learning Rate**: 0.3
- **Subsample**: 0.8
- **Training Data**: 1,296,675 transactions (100% fraudTrain.csv)

### ğŸ¯ Production Performance (Real-time)
- **Recall**: ~100% - PhÃ¡t hiá»‡n háº§u nhÆ° Táº¤T Cáº¢ giao dá»‹ch gian láº­n
- **Precision**: ~93% - 93% cáº£nh bÃ¡o lÃ  fraud thá»±c sá»±
- **False Positive Rate**: 0.8% - Ráº¥t tháº¥p (92/11,062 normal transactions)
- **Processing Latency**: <1s per transaction
- **Throughput**: 12.4 transactions/second

> ğŸ’¡ **CHÃš Ã**: Cassandra chá»‰ lÆ°u fraud alerts (prediction=1), KHÃ”NG lÆ°u táº¥t cáº£ giao dá»‹ch!  
> Xem chi tiáº¿t: [SYSTEM_EXPLANATION.md](./SYSTEM_EXPLANATION.md)

## ğŸš€ HÆ°á»›ng dáº«n cÃ i Ä‘áº·t vÃ  cháº¡y

### 1. Prerequisites

```bash
# YÃªu cáº§u há»‡ thá»‘ng
- Docker Desktop >= 20.10
- Docker Compose >= 2.0
- RAM >= 8GB (khuyáº¿n nghá»‹ 16GB)
- Disk space >= 20GB
- Windows 10/11 hoáº·c Linux
```

### 2. Clone repository vÃ  chuáº©n bá»‹ data

```powershell
# Clone project
git clone <repository-url>
cd hsbc-fraud-detection-new

# Kiá»ƒm tra data files
ls data/raw/
# Cáº§n cÃ³:
# - fraudTrain.csv (1.3M rows)
# - fraudTest.csv (0.5M rows)  
# - df_test_hdfs.csv (100K rows)
```

### 3. Khá»Ÿi Ä‘á»™ng há»‡ thá»‘ng

#### BÆ°á»›c 1: Start infrastructure services

```powershell
# Start Kafka, Zookeeper, Cassandra, MinIO
docker compose up -d zookeeper kafka cassandra minio
```

#### BÆ°á»›c 2: Táº¡o Kafka topic

```powershell
# Chá» Kafka khá»Ÿi Ä‘á»™ng hoÃ n toÃ n (30s)
Start-Sleep -Seconds 30

# Táº¡o topic
docker exec kafka kafka-topics --create `
  --topic transactions_hsbc `
  --bootstrap-server localhost:9092 `
  --partitions 3 `
  --replication-factor 1

# Verify topic
docker exec kafka kafka-topics --list --bootstrap-server localhost:9092
```

#### BÆ°á»›c 3: Setup Cassandra schema

```powershell
# Chá» Cassandra khá»Ÿi Ä‘á»™ng (60s)
Start-Sleep -Seconds 60

# Táº¡o keyspace
docker exec cassandra cqlsh -e "CREATE KEYSPACE IF NOT EXISTS hsbc WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1};"

# Táº¡o fraud_alerts table
docker exec cassandra cqlsh -e "CREATE TABLE IF NOT EXISTS hsbc.fraud_alerts (
    transaction_id text PRIMARY KEY,
    transaction_time timestamp,
    amount double,
    merchant text,
    category text,
    cc_num text,
    first text,
    last text,
    gender text,
    job text,
    state text,
    city text,
    zip text,
    is_fraud double,
    detected_at timestamp
);"

# Verify table
docker exec cassandra cqlsh -e "DESCRIBE TABLE hsbc.fraud_alerts;"
```

#### BÆ°á»›c 4: Setup MinIO buckets

```powershell
# Táº¡o bucket cho data lake
docker exec minio mc mb local/fraud-data-lake
docker exec minio mc mb local/fraud-models
```

#### BÆ°á»›c 5: Start Spark vÃ  train model

```powershell
# Start Spark cluster
docker compose up -d spark-master spark-worker

# Chá» Spark khá»Ÿi Ä‘á»™ng (30s)
Start-Sleep -Seconds 30

# Copy training scripts
docker cp streaming-pipeline/model_retraining_xgb.py spark-master:/opt/spark-apps/
docker cp streaming-pipeline/feature_engineering.py spark-master:/opt/spark-apps/
docker cp streaming-pipeline/config.py spark-master:/opt/spark-apps/

# Install dependencies trong spark-master
docker exec spark-master bash -c "pip3 install numpy xgboost scikit-learn pyarrow"

# Train XGBoost model (6-8 phÃºt)
docker exec spark-master bash -c "cd /opt/spark-apps && export PYSPARK_PYTHON=/usr/bin/python3 && /opt/spark/bin/spark-submit --master local[4] --driver-memory 4g --conf spark.sql.shuffle.partitions=20 /opt/spark-apps/model_retraining_xgb.py"
```

#### BÆ°á»›c 6: Start streaming pipeline

```powershell
# Copy streaming script
docker cp streaming-pipeline/unified_streaming.py spark-master:/opt/spark-apps/

# Start Spark streaming vá»›i XGBoost model
docker exec -d spark-master bash -c "cd /opt/spark-apps && export PYSPARK_PYTHON=/usr/bin/python3 && /opt/spark/bin/spark-submit --master local[4] --driver-memory 2g --conf spark.sql.shuffle.partitions=20 --conf spark.streaming.kafka.consumer.poll.ms=256 --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,org.apache.hadoop:hadoop-aws:3.3.4,com.datastax.spark:spark-cassandra-connector_2.12:3.4.0 /opt/spark-apps/unified_streaming.py"

# Verify streaming started
Start-Sleep -Seconds 10
docker logs spark-master --tail 20
```

#### BÆ°á»›c 7: Start producer vÃ  dashboard

```powershell
# Build vÃ  start producer
docker compose up -d --build producer

# Start dashboard
docker compose up -d dashboard

# Verify producer
docker logs producer --tail 50
```

### 4. Verify há»‡ thá»‘ng

```powershell
# Kiá»ƒm tra producer logs
docker logs producer --tail 20

# Kiá»ƒm tra fraud alerts trong Cassandra
docker exec cassandra cqlsh -e "SELECT COUNT(*) FROM hsbc.fraud_alerts;"

# Kiá»ƒm tra sample alerts
docker exec cassandra cqlsh -e "SELECT transaction_id, amount, merchant, is_fraud FROM hsbc.fraud_alerts LIMIT 10;"
```

### 5. Truy cáº­p Dashboard

Má»Ÿ browser vÃ  truy cáº­p:
- **Dashboard**: http://localhost:8501
- **Spark UI**: http://localhost:8080
- **MinIO Console**: http://localhost:9001 (admin/password123)

## ğŸ› Troubleshooting - CÃ¡c bug Ä‘Ã£ gáº·p vÃ  cÃ¡ch fix

### Bug 1: SparkXGBClassifier khÃ´ng cho phÃ©p set 'objective' parameter

**Lá»—i:**
```
ValueError: Setting custom 'objective' param is not allowed in 'SparkXGBClassifier'
```

**NguyÃªn nhÃ¢n:** SparkXGBClassifier tá»± Ä‘á»™ng cáº¥u hÃ¬nh `objective` cho binary classification.

**Giáº£i phÃ¡p:**
```python
# âŒ SAI - khÃ´ng Ä‘Æ°á»£c set objective manually
xgb = SparkXGBClassifier(
    objective='binary:logistic',  # DÃ²ng nÃ y gÃ¢y lá»—i
    n_estimators=100,
    ...
)

# âœ… ÄÃšNG - bá» objective parameter
xgb = SparkXGBClassifier(
    # objective auto-set by SparkXGBClassifier
    n_estimators=100,
    max_depth=6,
    learning_rate=0.3,
    ...
)
```

**File cáº§n fix:** `streaming-pipeline/model_retraining_xgb.py`

### Bug 2: ModuleNotFoundError - XGBoost not installed

**Lá»—i:**
```
ModuleNotFoundError: No module named 'xgboost'
```

**NguyÃªn nhÃ¢n:** XGBoost chÆ°a Ä‘Æ°á»£c cÃ i Ä‘áº·t trong Spark container.

**Giáº£i phÃ¡p:**
```powershell
# CÃ i Ä‘áº·t XGBoost vÃ  dependencies
docker exec spark-master bash -c "pip3 install xgboost xgboost[spark]"

# Verify installation
docker exec spark-master bash -c "python3 -c 'import xgboost; print(xgboost.__version__)'"
# Output: 2.1.4
```

### Bug 3: ImportError - scikit-learn required

**Lá»—i:**
```
ImportError: XGBoost requires scikit-learn to be installed
```

**NguyÃªn nhÃ¢n:** XGBoost phá»¥ thuá»™c vÃ o scikit-learn nhÆ°ng khÃ´ng Ä‘Æ°á»£c cÃ i cÃ¹ng.

**Giáº£i phÃ¡p:**
```powershell
# CÃ i scikit-learn
docker exec spark-master bash -c "pip3 install scikit-learn"

# Verify
docker exec spark-master bash -c "python3 -c 'import sklearn; print(sklearn.__version__)'"
# Output: 1.3.2
```

### Bug 4: ImportError - PyArrow must be installed

**Lá»—i:**
```
ImportError: PyArrow >= 4.0.0 must be installed; however, it was not found.
```

**NguyÃªn nhÃ¢n:** XGBoost sá»­ dá»¥ng PyArrow Ä‘á»ƒ xá»­ lÃ½ data vá»›i Spark, nhÆ°ng chÆ°a Ä‘Æ°á»£c cÃ i.

**Giáº£i phÃ¡p:**
```powershell
# CÃ i PyArrow
docker exec spark-master bash -c "pip3 install pyarrow"

# Verify
docker exec spark-master bash -c "python3 -c 'import pyarrow; print(pyarrow.__version__)'"
# Output: 17.0.0
```

### Bug 5: Column name mismatch - 'amt' vs 'amount'

**Lá»—i:**
```
AnalysisException: Column 'amt' does not exist
```

**NguyÃªn nhÃ¢n:** Feature engineering rename cá»™t `amt` thÃ nh `amount`, nhÆ°ng code váº«n dÃ¹ng tÃªn cÅ©.

**Giáº£i phÃ¡p:**
```python
# âŒ SAI - dÃ¹ng tÃªn cá»™t cÅ©
feature_cols = ['amt', 'age', 'city_pop', ...]

# âœ… ÄÃšNG - dÃ¹ng tÃªn sau khi rename
feature_cols = ['amount', 'age', 'city_pop', ...]
```

**File cáº§n fix:** Táº¥t cáº£ model training scripts.

### Bug 6: PySpark maxDepth limit

**Lá»—i:**
User request: `maxDepth=200` nhÆ°ng model warning vá» performance.

**NguyÃªn nhÃ¢n:** PySpark DecisionTree cÃ³ limit thá»±c táº¿ á»Ÿ maxDepth=30 Ä‘á»ƒ trÃ¡nh memory issues.

**Giáº£i phÃ¡p:**
```python
# âŒ KhÃ´ng khuyáº¿n nghá»‹
dtc = DecisionTreeClassifier(maxDepth=200)  # QuÃ¡ lá»›n, cÃ³ thá»ƒ OOM

# âœ… Khuyáº¿n nghá»‹
dtc = DecisionTreeClassifier(maxDepth=30)   # Optimal cho PySpark

# âœ… Tá»‘t hÆ¡n - dÃ¹ng ensemble methods
xgb = SparkXGBClassifier(max_depth=6, n_estimators=100)  # Tá»‘t hÆ¡n nhiá»u
```

### Bug 7: Model path not found

**Lá»—i:**
```
java.io.FileNotFoundException: /opt/data/models/fraud_dt_21features
```

**NguyÃªn nhÃ¢n:** Streaming pipeline tÃ¬m model cÅ© sau khi train model má»›i.

**Giáº£i phÃ¡p:**
```python
# Trong unified_streaming.py
def load_model(self):
    # Äá»•i path tá»« DecisionTree sang XGBoost
    model_path = "/opt/data/models/fraud_xgb_21features"  # âœ… XGBoost path
    # model_path = "/opt/data/models/fraud_dt_21features"  # âŒ Old path
```

### Bug 8: Container restart máº¥t Python packages

**Triá»‡u chá»©ng:** Sau khi restart container, XGBoost/PyArrow bÃ¡o lá»—i not found.

**NguyÃªn nhÃ¢n:** Packages Ä‘Æ°á»£c cÃ i vÃ o container runtime, khÃ´ng persist khi restart.

**Giáº£i phÃ¡p táº¡m thá»i:**
```powershell
# Re-install sau má»—i láº§n restart
docker exec spark-master bash -c "pip3 install xgboost scikit-learn pyarrow"
```

**Giáº£i phÃ¡p vÄ©nh viá»…n:** ThÃªm vÃ o Dockerfile cá»§a spark-master:
```dockerfile
# Trong Dockerfile
RUN pip3 install --no-cache-dir \
    xgboost==2.1.4 \
    scikit-learn==1.3.2 \
    pyarrow==17.0.0
```

## ğŸ“ Cáº¥u trÃºc dá»± Ã¡n

```
hsbc-fraud-detection-new/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â”œâ”€â”€ fraudTrain.csv          # Training data (1.3M rows)
â”‚   â”‚   â”œâ”€â”€ fraudTest.csv           # Test data (0.5M rows)
â”‚   â”‚   â””â”€â”€ df_test_hdfs.csv        # Streaming test data (100K rows)
â”‚   â””â”€â”€ processed/                  # Processed data
â”‚
â”œâ”€â”€ streaming-pipeline/
â”‚   â”œâ”€â”€ config.py                   # Configuration
â”‚   â”œâ”€â”€ feature_engineering.py      # 21 features engineering
â”‚   â”œâ”€â”€ unified_streaming.py        # Main streaming pipeline
â”‚   â”œâ”€â”€ model_retraining.py         # DecisionTree training
â”‚   â”œâ”€â”€ model_retraining_rf.py      # RandomForest training
â”‚   â””â”€â”€ model_retraining_xgb.py     # XGBoost training (ACTIVE)
â”‚
â”œâ”€â”€ producer/
â”‚   â”œâ”€â”€ producer.py                 # Kafka producer
â”‚   â”œâ”€â”€ config.py                   # Producer config
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ Dockerfile
â”‚
â”œâ”€â”€ models/
â”‚   â””â”€â”€ fraud_detection_rf/         # Saved models
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ setup_cassandra.sh          # Cassandra schema setup
â”‚   â”œâ”€â”€ run_initial_training.sh     # Initial model training
â”‚   â”œâ”€â”€ test_phase4.sh              # Phase 4 testing
â”‚   â””â”€â”€ upload_training_data.py     # Upload data to S3
â”‚
â”œâ”€â”€ dashboard/
â”‚   â”œâ”€â”€ app.py                      # Streamlit dashboard
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ Dockerfile
â”‚
â”œâ”€â”€ docker-compose.yml              # Docker orchestration
â”œâ”€â”€ Makefile                        # Build automation
â””â”€â”€ README.md                       # This file
```

## ğŸ”§ Configuration

### Kafka Configuration
```yaml
# docker-compose.yml
kafka:
  environment:
    KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'true'
    KAFKA_NUM_PARTITIONS: 3
    KAFKA_DEFAULT_REPLICATION_FACTOR: 1
```

### Spark Configuration
```python
# unified_streaming.py
spark.conf.set("spark.sql.shuffle.partitions", "20")
spark.conf.set("spark.streaming.kafka.consumer.poll.ms", "256")
spark.conf.set("spark.cassandra.connection.host", "cassandra")
```

### Producer Configuration
```python
# producer/config.py
KAFKA_BOOTSTRAP_SERVERS = 'kafka:29092'
TOPIC_NAME = 'transactions_hsbc'
TRANSACTION_RATE = 15  # transactions per second
BATCH_SIZE = 100
```

## ğŸ“ˆ Monitoring

### 1. Check streaming status
```powershell
# Producer status
docker logs producer --tail 20

# Spark streaming logs
docker logs spark-master --tail 50

# Fraud alerts count
docker exec cassandra cqlsh -e "SELECT COUNT(*) FROM hsbc.fraud_alerts;"
```

### 2. Performance metrics
```powershell
# Processing rate
docker exec spark-master bash -c "ps aux | grep unified_streaming"

# Memory usage
docker stats spark-master

# Kafka lag
docker exec kafka kafka-consumer-groups --bootstrap-server localhost:9092 --describe --group spark-streaming-group
```

## ğŸ¯ Model Training Workflow

### Train XGBoost model (Production)
```powershell
docker exec spark-master bash -c "cd /opt/spark-apps && export PYSPARK_PYTHON=/usr/bin/python3 && /opt/spark/bin/spark-submit --master local[4] --driver-memory 4g --conf spark.sql.shuffle.partitions=20 /opt/spark-apps/model_retraining_xgb.py"
```

### Train DecisionTree model
```powershell
docker exec spark-master bash -c "cd /opt/spark-apps && export PYSPARK_PYTHON=/usr/bin/python3 && /opt/spark/bin/spark-submit --master local[4] --driver-memory 4g /opt/spark-apps/model_retraining.py"
```

### Train RandomForest model
```powershell
docker exec spark-master bash -c "cd /opt/spark-apps && export PYSPARK_PYTHON=/usr/bin/python3 && /opt/spark/bin/spark-submit --master local[4] --driver-memory 4g /opt/spark-apps/model_retraining_rf.py"
```

## ğŸ§ª Testing

### Test producer
```powershell
# Send 1000 test transactions
docker logs producer --tail 20
```

### Test streaming pipeline
```powershell
# Check if streaming is processing
docker logs spark-master 2>&1 | Select-String "Processing"

# Check fraud detection rate
docker exec cassandra cqlsh -e "SELECT COUNT(*) FROM hsbc.fraud_alerts;"
```

## ğŸ“Š Expected Results

### Normal Operation
- **Producer rate**: ~12-15 tx/sec
- **Processing latency**: <1 second
- **Fraud detection rate**: ~10% of total transactions
- **Model accuracy**: AUC-ROC 0.9964
- **False positive rate**: <0.5%

### Sample Output
```
=== PRODUCER STATUS ===
ğŸ“Š Sent: 2,400 | Fraud: 243 (10.12%) | Errors: 0 | Rate: 12.3 tx/sec

=== FRAUD ALERTS COUNT ===
count: 184

=== DETECTION RATE ===
76% fraud transactions detected (184/243)
```

## ğŸ›‘ Shutdown System

### Graceful shutdown
```powershell
# Stop producer first
docker compose stop producer

# Wait for streaming to finish current batch (30s)
Start-Sleep -Seconds 30

# Stop streaming
docker exec spark-master bash -c "pkill -f unified_streaming"

# Stop all services
docker compose down
```

### Clean shutdown (remove all data)
```powershell
# Stop and remove containers
docker compose down -v

# Remove all data volumes
docker volume prune -f
```

## ğŸ”„ Restart After Shutdown

```powershell
# Start infrastructure
docker compose up -d zookeeper kafka cassandra minio spark-master spark-worker

# Wait for services
Start-Sleep -Seconds 60

# Re-install Python packages (if needed)
docker exec spark-master bash -c "pip3 install xgboost scikit-learn pyarrow"

# Start streaming
docker exec -d spark-master bash -c "cd /opt/spark-apps && export PYSPARK_PYTHON=/usr/bin/python3 && /opt/spark/bin/spark-submit --master local[4] --driver-memory 2g --conf spark.sql.shuffle.partitions=20 --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,org.apache.hadoop:hadoop-aws:3.3.4,com.datastax.spark:spark-cassandra-connector_2.12:3.4.0 /opt/spark-apps/unified_streaming.py"

# Start producer
docker compose up -d producer dashboard
```

## ğŸ“ Best Practices

1. **Always wait for services to be ready** before starting dependent services
2. **Monitor logs regularly** to catch issues early
3. **Backup Cassandra data** before major changes
4. **Use consistent feature engineering** across training and streaming
5. **Version control your models** with metadata
6. **Test with small data first** before full production

## ğŸ“ Support

For issues or questions:
1. Check logs: `docker logs <container_name>`
2. Review this troubleshooting guide
3. Check Spark UI: http://localhost:8080
4. Verify all containers running: `docker compose ps`

## ğŸ“ License

Copyright Â© 2025 HSBC Fraud Detection Team
